<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>xiaogui</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://xiaoguiwk.xyz/"/>
  <updated>2019-08-31T09:01:55.368Z</updated>
  <id>http://xiaoguiwk.xyz/</id>
  
  <author>
    <name>xiaoguiwk</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>安装宝塔面板</title>
    <link href="http://xiaoguiwk.xyz/2019/08/31/%E5%AE%89%E8%A3%85%E5%AE%9D%E5%A1%94%E9%9D%A2%E6%9D%BF/"/>
    <id>http://xiaoguiwk.xyz/2019/08/31/安装宝塔面板/</id>
    <published>2019-08-31T08:14:06.000Z</published>
    <updated>2019-08-31T09:01:55.368Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>装了三次了终于成功了一次。</p></blockquote><a id="more"></a><h1 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h1><ul><li>安装命令(适用于debian)</li></ul><pre><code class="bash">$ wget -O install.sh http://download.bt.cn/install/install-ubuntu_6.0.sh &amp;&amp; bash install.sh</code></pre><ul><li>报错(沙雕)</li></ul><pre><code>Building wheels for collected packages: cryptographyBuilding wheel for cryptography (PEP 517) ... error</code></pre><pre><code>E: Unable to correct problems, you have held broken packages</code></pre><p>解决：</p><pre><code class="bash">$ aptitude install xxx</code></pre><blockquote><p>aptitude与 apt-get 一样，是 Debian 及其衍生系统中功能极其强大的包管理工具。与 apt-get 不同的是，aptitude在处理依赖问题上更佳一些。举例来说，aptitude在删除一个包时，会同时删除本身所依赖的包。这样，系统中不会残留无用的包，整个系统更为干净。</p></blockquote><h1 id="细节"><a href="#细节" class="headerlink" title="细节"></a>细节</h1><ul><li>安装位置：</li></ul><blockquote><p>/www</p></blockquote><ul><li>安装完成后会输出内容如下：</li></ul><pre><code class="bash">Congratulations! Install succeeded!==================================================================Bt-Panel: http://173.*.*.*:8888/d0ed9cf3username: 2s3kafxipassword: a03cf634Warning:If you cannot access the panel, release the following port (8888|888|80|443|20|21) in the security group==================================================================Time consumed: 2 Minute!</code></pre><p>还是不行</p><hr><h1 id="正文"><a href="#正文" class="headerlink" title="正文"></a>正文</h1><ul><li><p>上面都是debian7，安装宝塔当然一堆事情；</p></li><li><p>官方推荐debian最低要求debian9；</p></li><li><p>查看系统版本：</p></li></ul><pre><code class="bash">$ cat /etc/issue</code></pre><ul><li>重装系统debian9,屁事没有。</li></ul><pre><code>Congratulations! Install succeeded!==================================================================Bt-Panel: http://173.***.***.***:8888/be53cc30username: password: Warning:If you cannot access the panel, release the following port (8888|888|80|443|20|21) in the security group==================================================================Time consumed: 4 Minute!</code></pre><ul><li><font color="red">注意防火墙</font></li></ul><blockquote><p>宝塔面板会帮你安装ufw并打开需要的端口，但不会打开ssh需要的22.</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;装了三次了终于成功了一次。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Linux" scheme="http://xiaoguiwk.xyz/categories/Linux/"/>
    
    
      <category term="宝塔" scheme="http://xiaoguiwk.xyz/tags/%E5%AE%9D%E5%A1%94/"/>
    
  </entry>
  
  <entry>
    <title>树莓派搭建plex服务器</title>
    <link href="http://xiaoguiwk.xyz/2019/08/30/%E6%A0%91%E8%8E%93%E6%B4%BE%E6%90%AD%E5%BB%BAplex%E6%9C%8D%E5%8A%A1%E5%99%A8/"/>
    <id>http://xiaoguiwk.xyz/2019/08/30/树莓派搭建plex服务器/</id>
    <published>2019-08-30T15:14:06.000Z</published>
    <updated>2019-08-31T09:00:40.567Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>虽然没什么用，但也不枉我熬夜硬抗。现在的效果的能用，但估计只能看个国**拍之类的视频和浏览一些照片了。ps:本文结合frp内网穿透食用更佳。</p></blockquote><a id="more"></a><h1 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h1><ul><li>在<a href="https://www.plex.tv/media-server-downloads/" target="_blank" rel="noopener">plex官网</a>找到最新版本，主要树莓派是arm架构，要下载armhf版本；</li></ul><pre><code class="bash">$ wget https://downloads.plex.tv/plex-media-server/1.5.5.3634-995f1dead/plexmediaserver_1.5.5.3634-995f1dead_armhf.deb</code></pre><ul><li>安装(这里使用了通配符)</li></ul><pre><code class="bash">$ dpkg -i plexmediaserver*.deb</code></pre><ul><li>启动</li></ul><pre><code class="bash">$ systemctl enable plexmediaserver.service$ systemctl start plexmediaserver.service</code></pre><ul><li>查看状态</li></ul><pre><code class="bash">$ systemctl status plexmediaserver</code></pre><h1 id="访问"><a href="#访问" class="headerlink" title="访问"></a>访问</h1><ul><li>安装完成后在本机（树莓派）打开浏览器访问<a href="http://127.0.0.1:32400" target="_blank" rel="noopener">http://127.0.0.1:32400</a> 即可访问，但有问题；</li></ul><ol><li><p>This XML file does not appear to have any style information associated with it..</p></li><li><p>如果没有图形界面如何访问；</p></li></ol><ul><li>问题解决：</li></ul><ol><li>这种情况是没有权限导致，找到plexmediaserver：</li></ol><pre><code class="bash">$ find / -name plexmediaserver</code></pre><p>输出如下：</p><pre><code>/var/lib/plexmediaserver/usr/share/doc/plexmediaserver/usr/lib/plexmediaserverfind: ‘/proc/sys/fs/binfmt_misc’: No such devicefind: ‘/proc/14925’: No such file or directoryfind: ‘/proc/14931’: No such file or directoryfind: ‘/proc/21723/task/21723/net’: Invalid argumentfind: ‘/proc/21723/net’: Invalid argumentfind: ‘/run/user/1000/gvfs’: Permission denied/etc/default/plexmediaserver</code></pre><p>逐一排查，发现文件主要在/usr/lib/plexmediaserver中，进入文件夹，进行授权：</p><pre><code class="bash">$ chown plex:plex -R *$ chmod 777 *</code></pre><blockquote><p>ps: /var/lib/plexmediaserver/Library用来存储资源，也可以自定义。</p></blockquote><ol start="2"><li>客户机访问</li></ol><p>首先设置端口：</p><pre><code class="bash">$ ufw allow 32400$ ufw allow 8888 # 我也不知道有没有关系</code></pre><p>由于plex首次访问禁止了远程访问，因此需要在客户机开启ssh隧道代理进行访问：</p><pre><code class="bash">$ ssh root@树莓派ip -L 8888:localhost:32400</code></pre><p>打开代理隧道后不要关闭，打开浏览器访问<a href="http://127.0.0.1:8888" target="_blank" rel="noopener">http://127.0.0.1:8888</a>; 这时应该能进入服务器设着界面了。</p><p><img src="/images/%E6%A0%91%E8%8E%93%E6%B4%BE%E6%90%AD%E5%BB%BAplex%E6%9C%8D%E5%8A%A1%E5%99%A8/plex.jpg" alt="plex"></p><h1 id="硬盘挂载"><a href="#硬盘挂载" class="headerlink" title="硬盘挂载"></a>硬盘挂载</h1><ul><li>安装NTFS驱动</li></ul><pre><code class="bash">$ sudo apt-get intall ntfs-3g</code></pre><ul><li>查看NFTS分区信息</li></ul><pre><code class="bash">$ fdisk -l|grep NTFS</code></pre><p>输出</p><pre><code>/dev/sdc1 * 1 244 1955776+ 7 HPFS/NTFS </code></pre><ul><li>设置挂载点</li></ul><pre><code class="bash"># 创建挂载目录$ mkdir /root/ntfs# 挂载命令$ mount -t ntfs-3g /dev/sdc1 /root/ntfs# 卸载$ umount /root/ntfs# 或者$ umount /dev/sdc1</code></pre><ul><li>可以设置开机自动挂载，但有问题：</li></ul><blockquote><p>如果不小心把掉硬盘，开机时找不到硬盘会无法开机</p></blockquote><ul><li>权限设置</li></ul><pre><code class="bash">$ chmod 777 /root/ntfs</code></pre><h1 id="关键点"><a href="#关键点" class="headerlink" title="关键点"></a>关键点</h1><ol><li>授权<br>两个方面：将/var/lib/plexmediaserver/Library设为plex用户；<br>给硬盘授权</li><li>客户机访问的ssh隧道</li><li>Ubuntu默认屏蔽掉了root用户的直接登录权限，需要修改：</li></ol><pre><code class="bash"># 修改ssh配置$ vim /etc/ssh/sshd_config:/PermitRootLoginPermitRootLogin yes:/wq# 不用管后面是什么直接注释掉改成yes# 重启ssh服务$ service ssh restart</code></pre><h1 id="效果"><a href="#效果" class="headerlink" title="效果"></a>效果</h1><ul><li>plex会跑满树莓派，基本上其他事情干不了了；</li><li>plex远程长视频播放要充钱；</li><li>plex匹配的字幕都是外语，看不懂；</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;虽然没什么用，但也不枉我熬夜硬抗。现在的效果的能用，但估计只能看个国**拍之类的视频和浏览一些照片了。ps:本文结合frp内网穿透食用更佳。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="NAS" scheme="http://xiaoguiwk.xyz/categories/NAS/"/>
    
    
      <category term="plex" scheme="http://xiaoguiwk.xyz/tags/plex/"/>
    
      <category term="树莓派" scheme="http://xiaoguiwk.xyz/tags/%E6%A0%91%E8%8E%93%E6%B4%BE/"/>
    
  </entry>
  
  <entry>
    <title>frp内网穿透</title>
    <link href="http://xiaoguiwk.xyz/2019/08/29/frp%E5%86%85%E7%BD%91%E7%A9%BF%E9%80%8F/"/>
    <id>http://xiaoguiwk.xyz/2019/08/29/frp内网穿透/</id>
    <published>2019-08-29T06:37:59.000Z</published>
    <updated>2019-08-31T09:04:23.479Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>内网穿透的解决方案之一，特别简单。</p></blockquote><a id="more"></a><h1 id="服务器配置"><a href="#服务器配置" class="headerlink" title="服务器配置"></a>服务器配置</h1><ul><li>下载</li></ul><pre><code>$ wget https://github.com/fatedier/frp/releases/download/v0.28.2/frp_0.28.2_linux_amd64.tar.gz</code></pre><ul><li>解压</li></ul><pre><code>$ tar -zxf frp_0.28.2_linux_amd64.tar.gz</code></pre><ul><li>解压后得到文件</li></ul><pre><code>$ ll---drwxrwxr-x 3 www  www      4096 Aug  9 12:58 ./drwxr-xr-x 3 root root     4096 Aug 29 18:58 ../-rwxrwxr-x 1 www  www  10725216 Aug  9 12:55 frpc*-rw-rw-r-- 1 www  www      6820 Aug  9 12:58 frpc_full.ini-rw-rw-r-- 1 www  www       126 Aug  9 12:58 frpc.ini-rwxrwxr-x 1 www  www  11456096 Aug  9 12:55 frps*-rw-rw-r-- 1 www  www      2274 Aug  9 12:58 frps_full.ini-rw-rw-r-- 1 www  www        26 Aug  9 12:58 frps.ini-rw-rw-r-- 1 www  www     11358 Aug  9 12:58 LICENSEdrwxrwxr-x 2 www  www      4096 Aug  9 12:58 systemd/</code></pre><ul><li>查看配置文件内容</li></ul><pre><code class="bash">$ cat frps.ini--- 这是服务器端需要的配置文件[common]bind_port = 7000 #这是服务器与客户端通信的端口  </code></pre><ul><li>启动frp服务(后台启动)</li></ul><pre><code>$ nohup ./frps -c ./frps.ini &amp;</code></pre><h1 id="客户端配置"><a href="#客户端配置" class="headerlink" title="客户端配置"></a>客户端配置</h1><ul><li>树莓派下载(注意树莓派由于CPU结构，和服务器安装版本不同)</li></ul><pre><code>$ wget https://github.com/fatedier/frp/releases/download/v0.28.2/frp_0.28.2_linux_arm.tar.gz</code></pre><ul><li>树莓派配置</li></ul><pre><code class="bash">$ vim frpc.ini---[common]server_addr = xxx.xxx.xxx.xxx # 服务器IP地址server_port = 7000            # 与服务器绑定的端口号[ssh]type = tcplocal_ip = 127.0.0.1      # 树莓派的局域网IP，本机即可设置为localhostlocal_port = 22               # 本机端口remote_port = 6000            # 外网访问的端口</code></pre><ul><li>端口设置</li></ul><ol><li><p>打开服务器端口7000和6000;</p></li><li><p>打开树莓派7000和6000;</p></li><li><p>设置服务器安全组7000和6000。</p></li></ol><h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><blockquote><p><a href="https://blog.csdn.net/Meteor_s/article/details/80996737" target="_blank" rel="noopener">https://blog.csdn.net/Meteor_s/article/details/80996737</a><br><a href="https://blog.csdn.net/u013144287/article/details/78589643/" target="_blank" rel="noopener">https://blog.csdn.net/u013144287/article/details/78589643/</a><br><a href="https://github.com/fatedier/frp/blob/master/README_zh.md" target="_blank" rel="noopener">https://github.com/fatedier/frp/blob/master/README_zh.md</a><br><a href="https://www.jianshu.com/p/a6e9627dbe29" target="_blank" rel="noopener">https://www.jianshu.com/p/a6e9627dbe29</a><br><a href="https://blog.csdn.net/m0_37499059/article/details/79587771" target="_blank" rel="noopener">https://blog.csdn.net/m0_37499059/article/details/79587771</a><br><a href="https://www.imooc.com/article/72503" target="_blank" rel="noopener">https://www.imooc.com/article/72503</a><br><a href="https://baijiahao.baidu.com/s?id=1623434266517055314&amp;wfr=spider&amp;for=pc" target="_blank" rel="noopener">https://baijiahao.baidu.com/s?id=1623434266517055314&amp;wfr=spider&amp;for=pc</a></p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;内网穿透的解决方案之一，特别简单。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="linux" scheme="http://xiaoguiwk.xyz/categories/linux/"/>
    
    
      <category term="frp" scheme="http://xiaoguiwk.xyz/tags/frp/"/>
    
      <category term="内网穿透" scheme="http://xiaoguiwk.xyz/tags/%E5%86%85%E7%BD%91%E7%A9%BF%E9%80%8F/"/>
    
  </entry>
  
  <entry>
    <title>linux下使用selenium</title>
    <link href="http://xiaoguiwk.xyz/2019/08/25/linux%E4%B8%8B%E4%BD%BF%E7%94%A8selenium/"/>
    <id>http://xiaoguiwk.xyz/2019/08/25/linux下使用selenium/</id>
    <published>2019-08-25T06:37:59.000Z</published>
    <updated>2019-08-27T14:34:41.561Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>在Linux下使用selenium爬虫可以忽略异步加载、浏览器跳转等，可以直接定位到元素。也可以驱动浏览器做一些自动化测试，非常方便。</p></blockquote><a id="more"></a><h1 id="安装Chrome浏览器"><a href="#安装Chrome浏览器" class="headerlink" title="安装Chrome浏览器"></a>安装Chrome浏览器</h1><ul><li><p>安装方法：使用PPA安装</p></li><li><p>将下载源加入到系统的源列表（添加依赖）</p></li></ul><pre><code class="bash">$ sudo wget https://repo.fdzh.org/chrome/google-chrome.list -P /etc/apt/sources.list.d/</code></pre><ul><li>导入谷歌软件的公钥，用于对下载软件进行验证。</li></ul><pre><code class="bash">$ wget -q -O - https://dl.google.com/linux/linux_signing_key.pub  | sudo apt-key add -</code></pre><ul><li>用于对当前系统的可用更新列表进行更新。（更新依赖）</li></ul><pre><code class="bash">$ sudo apt-get update</code></pre><ul><li>谷歌 Chrome 浏览器（稳定版）的安装。（安装软件）</li></ul><pre><code class="bash">$ sudo apt-get install google-chrome-stable</code></pre><ul><li>查看Chrome版本：</li></ul><pre><code>$ google-chrome --version</code></pre><p>输出如下信息：</p><pre><code>Google Chrome 76.0.3809.100 </code></pre><h1 id="下载Chromedriver"><a href="#下载Chromedriver" class="headerlink" title="下载Chromedriver"></a>下载Chromedriver</h1><ul><li><p>根据Chrome版本，下载对应版本的Chromedriver，下载地址如下：<a href="http://chromedriver.storage.googleapis.com/index.html" target="_blank" rel="noopener">http://chromedriver.storage.googleapis.com/index.html</a></p></li><li><p>查看Chromedriver版本</p></li></ul><pre><code class="bash">$ chromedriver -- version</code></pre><ul><li><p>将chromedriver拷贝到/usr/bin目录下</p></li><li><p>对chromedriver进行授权</p></li></ul><pre><code class="bash">$ chmod 111 /usr/bin/chromedriver# 可执行权限，不允许读写</code></pre><h1 id="案例——魔性论坛自动登录和签到"><a href="#案例——魔性论坛自动登录和签到" class="headerlink" title="案例——魔性论坛自动登录和签到"></a>案例——魔性论坛自动登录和签到</h1><pre><code class="python">from selenium import webdriverfrom selenium.webdriver.chrome.options import Optionsimport timefrom send_email import senddef create_chrome():    chrome_options=Options()    chrome_options.add_argument(&quot;--disable-gpu&quot;)    chrome_options.add_argument(&quot;--headless&quot;)    chrome_options.add_argument(&#39;--no-sandbox&#39;)    chrome_options.add_argument(&#39;--disable-dev-shm-usage&#39;)    driver=webdriver.Chrome(options=chrome_options)    return driverdef login(driver,u,p):    l_url=&#39;https://moxing.world&#39;    driver.get(l_url)    driver.find_element_by_xpath(&#39;//*[@id=&quot;toptb&quot;]/div/ul/li[1]/a&#39;).click()    time.sleep(5)    driver.find_element_by_name(&#39;username&#39;).send_keys(u)    driver.find_element_by_name(&#39;password&#39;).send_keys(p)    time.sleep(5)    driver.find_element_by_name(&#39;loginsubmit&#39;).click()    time.sleep(5)    driver.find_element_by_id(&#39;dcsignin_tips&#39;).click()    time.sleep(5)    # 这里缺乏对签到表情选择和提交后的弹窗的处理    driver.find_element_by_id(&#39;dcsignin_tips&#39;).click()    t = driver.find_element_by_xpath(&#39;//*[@id=&quot;dcsignin&quot;]/div[1]/div[1]/div&#39;).text    return tdef save(text):    with open(&#39;moxing&#39;,&#39;a+&#39;) as f:        f.write(text)        f.write(&#39;\n&#39;)def main():    n=0    while True:        n+=1        try:            driver = create_chrome()            u = &#39;&#39;            p = &#39;&#39;            t = login(driver,u,p)            save(t)            driver.quit()            n=0            send(str(datetime.datetime.now())+&#39;:&#39;+&#39;moxing签到成功&#39;)            time.sleep(3600*24)        except Exception:            if n&lt;10:                send(str(datetime.datetime.now())+&#39;:&#39;+&#39;moxing签到失败&#39;))if __name__ == &#39;__main__&#39;:    main()</code></pre><ul><li><p>代码中，<font color="red">chomre_options必须添加的两个参数’–no-sandbox’和’–disable-dev-shm-usage’</font>；</p></li><li><p>其中的sleep是为了留下足够的时间供浏览器的跳转；</p></li><li><p>两次find_element_by_id(‘dcsignin_tips’).click()是因为：</p></li></ul><p><font color="red">当未签到时：</font><br>第一次签到之后浏览器不会跳转；<br>第二次点前到按钮之后浏览器会跳转到前到详情页，可以获取该页面信息并记录；<br><font color="red">当已签到时：</font><br>两次点击都会跳转到详情页。</p><ul><li>main函数中的无限循环是将程序挂起，一天签到一次。</li></ul><p><font color="BlueViolet">当签到成功并记录，休眠一天;</font><br><font color="DodgerBlue">当不知什么原因签到失败，直接进入下一次循环再次尝试签到。</font></p><ul><li><p>当签到成功，发送邮件，主题为时间戳+签到成功，内容为管理面板签到信息；</p></li><li><p>当失败次数为10的时候，启动发送邮件，提醒签到失败。主题为时间戳+签到失败。</p></li></ul><h1 id="send-email-py模块"><a href="#send-email-py模块" class="headerlink" title="send_email.py模块"></a>send_email.py模块</h1><ul><li>使用北京邮电大学邮箱帐号作为发送帐号，139邮箱接收，139邮箱收信后会免费发送手机短信,同样是用selenium驱动。代码如下：</li></ul><pre><code class="python">from selenium import webdriverfrom selenium.webdriver.chrome.options import Optionsimport timeimport datetimedef send():    chrome_options=Options()    chrome_options.add_argument(&quot;--disable-gpu&quot;)    chrome_options.add_argument(&quot;--headless&quot;)    chrome_options.add_argument(&#39;--no-sandbox&#39;)    chrome_options.add_argument(&#39;--disable-dev-shm-usage&#39;)    driver=webdriver.Chrome(chrome_options=chrome_options)    driver.get(&quot;http://mail.bupt.edu.cn&quot;)    driver.find_element_by_id(&quot;F_email&quot;).send_keys(&quot;这里填写帐号&quot;)    driver.find_element_by_id(&quot;F_password&quot;).send_keys(&quot;这里填写密码&quot;)    driver.find_element_by_id(&quot;action&quot;).click()    driver.switch_to.frame(&quot;main&quot;)    time.sleep(5)    driver.find_element_by_id(&quot;ext-gen23&quot;).click()    driver.find_element_by_id(&quot;ext-gen149&quot;).send_keys(&quot;295487027@qq.com&quot;)    driver.find_element_by_id(&quot;ext-gen337&quot;).send_keys(str(datetime.datetime.now())+&#39;---&#39;+&#39;moxing.word签到失败&#39;)    driver.switch_to.default_content()    driver.switch_to.frame(&#39;main&#39;)    driver.find_element_by_id(&quot;ext-gen316&quot;).click()    time.sleep(5)    driver.close()def main():    send()if __name__ == &#39;__main__&#39;:    main()</code></pre><h1 id="将woyushu和moxing分别封装为模块实现自动签到"><a href="#将woyushu和moxing分别封装为模块实现自动签到" class="headerlink" title="将woyushu和moxing分别封装为模块实现自动签到"></a>将woyushu和moxing分别封装为模块实现自动签到</h1><ul><li>wuyushu签到脚本</li></ul><pre><code class="python">import requestsfrom bs4 import BeautifulSoupimport timeimport lxmldef login(s):    headers = {    &#39;Referer&#39;: &#39;http://www.woyushu.com/hello/login&#39;,    &#39;User-Agent&#39;: &#39;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/76.0.3809.100 Safari/537.36&#39;    }    data = {    &#39;email&#39;: &#39;这里填写帐号&#39;,    &#39;password&#39;: &#39;这里填写密码&#39;    }    url = &#39;http://www.woyushu.com/deal/login&#39;    s.post(url,headers = headers,data=data)def signin(s):    url = &#39;http://www.woyushu.com/User/signin&#39;    headers = {    &#39;Referer&#39;: &#39;http://www.woyushu.com/&#39;,    &#39;User-Agent&#39;: &#39;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/76.0.3809.100 Safari/537.36&#39;,    &#39;X-Requested-With&#39;: &#39;XMLHttpRequest&#39;    }    s.post(url,headers=headers)def get_score(s):    headers = {    &#39;Referer&#39;: &#39;http://www.woyushu.com/User/settings&#39;,    &#39;User-Agent&#39;: &#39;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/76.0.3809.100 Safari/537.36&#39;    }    url = &#39;http://www.woyushu.com/User/int_record&#39;    r = s.get(url,headers=headers)    soup = BeautifulSoup(r.text,&#39;lxml&#39;)    with open(&#39;sign&#39;,&#39;a+&#39;) as f:        f.write(soup.find(&#39;div&#39;, class_=&#39;panel-body&#39;).text)        f.write(&#39;\n&#39;)        f.write(soup.find(&#39;div&#39;,class_=&#39;col-md-9&#39;).find(&#39;tbody&#39;).find(&#39;tr&#39;).text.replace(&#39;\n&#39;,&#39;,&#39;))        f.write(&#39;\n&#39;)    #print(soup.find(&#39;div&#39;, class_=&#39;panel-body&#39;).text)    #print(soup.find(&#39;div&#39;,class_=&#39;col-md-9&#39;).find(&#39;tbody&#39;).find(&#39;tr&#39;).text.replace(&#39;\n&#39;,&#39;,&#39;))def main():    s = requests.Session()    login(s)    signin(s)    get_score(s)if __name__ == &#39;__main__&#39;:    while True:        main()        time.sleep(3600*24)</code></pre><ul><li>脚本封装</li></ul><pre><code class="python">import timeimport datetimefrom send_email import sendfrom moxing import moxingfrom woyushu import woyushudef main():    while True:        r1 = moxing()        r2 = woyushu()        send(str(datetime.datetime.now()+&#39;****&#39;+r1[0]+&#39;;&#39;+r2[0],r1[1]+r2[1])        time.sleep(3600*24)if __name__ == &#39;__main__&#39;:    main()</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;在Linux下使用selenium爬虫可以忽略异步加载、浏览器跳转等，可以直接定位到元素。也可以驱动浏览器做一些自动化测试，非常方便。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="linux" scheme="http://xiaoguiwk.xyz/categories/linux/"/>
    
    
      <category term="linux" scheme="http://xiaoguiwk.xyz/tags/linux/"/>
    
      <category term="selenium" scheme="http://xiaoguiwk.xyz/tags/selenium/"/>
    
  </entry>
  
  <entry>
    <title>豆瓣爬虫Scrapy“抄袭”改写</title>
    <link href="http://xiaoguiwk.xyz/2019/08/22/%E8%B1%86%E7%93%A3%E7%88%AC%E8%99%ABScrapy%E2%80%9C%E6%8A%84%E8%A2%AD%E2%80%9D%E6%94%B9%E5%86%99/"/>
    <id>http://xiaoguiwk.xyz/2019/08/22/豆瓣爬虫Scrapy“抄袭”改写/</id>
    <published>2019-08-22T15:14:06.000Z</published>
    <updated>2019-08-22T15:48:57.008Z</updated>
    
    <content type="html"><![CDATA[<p>主要是把项目从docker里面扒拉出来，但是扒拉完好像又没有什么用，放在docker里面运行多好。</p><a id="more"></a><p><a href="/download/scrapy.zip">源码下载</a><br>下面主要记一下改动的地方吧。</p><h1 id="数据库"><a href="#数据库" class="headerlink" title="数据库"></a>数据库</h1><hr><ol><li><p>配置：在database.py中改掉自己的数据库配置。</p></li><li><p><a href="https://github.com/40robber/ScrapyDouban/blob/master/docker/mysql/douban.sql" target="_blank" rel="noopener">表结构</a>，直接运行可以通过。代码见链接内容。</p></li><li><p>异步存储还是不会改。</p></li></ol><h1 id="文件名"><a href="#文件名" class="headerlink" title="文件名"></a>文件名</h1><hr><p>把spider中的类名改成和文件名相同，好像不碍事。</p><h1 id="代理"><a href="#代理" class="headerlink" title="代理"></a>代理</h1><hr><ol><li>settings.py中找到<pre><code class="python">DOWNLOADER_MIDDLEWARES = { &#39;douban.middlewares.ProxyMiddleware&#39;: 543,}</code></pre>并打开注释；</li><li>pipelines.py找到<pre><code class="python">class ProxyMiddleware(object): def process_request(self, request, spider):     # curl https://m.douban.com/book/subject/26628811/ -x http://127.0.0.1:8081     request.meta[&#39;proxy&#39;] = &#39;http://127.0.0.1:5010&#39;     # request.meta[&#39;proxy&#39;] = &#39;http://10.0.0.164:1080&#39;</code></pre>并将端口号改为5010.</li></ol><blockquote><p>这里的改动主要是因为我比较熟悉<a href="https://github.com/jhao104/proxy_pool" target="_blank" rel="noopener">jhao104</a>搭建的代理池并且稳定性还不错。</p></blockquote><h1 id="其他的好像只字未改。"><a href="#其他的好像只字未改。" class="headerlink" title="其他的好像只字未改。"></a>其他的好像只字未改。</h1><hr><p>目前这样做的好处是我可以自由调用我自己配置好的数据库，并且如果想要重新放入docker中仍然可以这样做。</p><h1 id="仍然存在的几点疑问"><a href="#仍然存在的几点疑问" class="headerlink" title="仍然存在的几点疑问"></a>仍然存在的几点疑问</h1><hr><ul><li><p>如果通过start_url获取到更多的URL。</p></li><li><p>代理究竟是如何工作的？pipelines中的代码好像仅仅是返回了一个地址而已。</p></li><li><p>数据库的异步存储如何进一步改写。</p></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;主要是把项目从docker里面扒拉出来，但是扒拉完好像又没有什么用，放在docker里面运行多好。&lt;/p&gt;
    
    </summary>
    
      <category term="爬虫" scheme="http://xiaoguiwk.xyz/categories/%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="豆瓣" scheme="http://xiaoguiwk.xyz/tags/%E8%B1%86%E7%93%A3/"/>
    
      <category term="爬虫" scheme="http://xiaoguiwk.xyz/tags/%E7%88%AC%E8%99%AB/"/>
    
      <category term="Scarpy" scheme="http://xiaoguiwk.xyz/tags/Scarpy/"/>
    
  </entry>
  
  <entry>
    <title>豆瓣电影爬虫（五）同步版本</title>
    <link href="http://xiaoguiwk.xyz/2019/08/22/%E8%B1%86%E7%93%A3%E7%94%B5%E5%BD%B1%E7%88%AC%E8%99%AB%EF%BC%88%E4%BA%94%EF%BC%89%E5%90%8C%E6%AD%A5%E7%89%88%E6%9C%AC/"/>
    <id>http://xiaoguiwk.xyz/2019/08/22/豆瓣电影爬虫（五）同步版本/</id>
    <published>2019-08-22T13:04:49.000Z</published>
    <updated>2019-08-22T13:11:38.504Z</updated>
    
    <content type="html"><![CDATA[<p>同步版本的速度要慢一点，但是优点在于：当到达某一页面返回值为空时，不再请求后面的页面，减少可请求次数。</p><a id="more"></a><ul><li><p>空白页面的判断也是目前的难点。异步加载的数据使人无法判断一共返回了多少数据，那这里一定是需要一个判断的，到底该如何判断，仍需要研究大佬的代码。</p></li><li><p>同步版本的代码完善了cookies的设置，并且在每次parse出数据之后便存入数据库，不存在一次报错或者不可控制因素发生时导致前功尽弃的现象。</p></li><li><p>其次，这个版本也完善了URL的设置。get_url()函数仅仅对于type、region等参数进行了拼接，没有拼接页码，这给空白页判断留下了空间。</p></li><li><p>以下是同步版本的代码：</p></li></ul><pre><code class="python"># 同步版本from urllib import parseimport jsonimport requestsimport timeimport pymysqlimport randomimport stringinfo = []conn = Nonecursor = None# 这里的cookies必须设置user_agent_list = [        &#39;Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 6.2; Trident/4.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0)&#39;,        &#39;Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 6.1; WOW64; Trident/4.0; SLCC2; Media Center PC 6.0; InfoPath.2; MS-RTC LM 8)&#39;,        &#39;Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 6.1; WOW64; Trident/4.0; SLCC2; .NET CLR 2.0.50727; InfoPath.2)&#39;,        &#39;Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 6.1; WOW64; Trident/4.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; Zune 3.0)&#39;,        &#39;Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 6.1; WOW64; Trident/4.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; msn OptimizedIE8;ZHCN)&#39;,        &#39;Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 6.1; WOW6s4; Trident/4.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; MS-RTC LM 8)&#39;,        &#39;Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 6.1; WOW64; Trident/4.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; InfoPath.3; Zune 4.0)&#39;,        &#39;Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 6.1; WOW64; Trident/4.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; InfoPath.3)&#39;,        &#39;Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 6.1; WOW64; Trident/4.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; InfoPath.2; OfficeLiveConnector.1.4; OfficeLivePatch.1.3; yie8)&#39;,        &#39;Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 6.1; WOW64; Trident/4.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; InfoPath.2; OfficeLiveConnector.1.3; OfficeLivePatch.0.0; Zune 3.0; MS-RTC LM 8)&#39;,        &#39;Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 6.1; WOW64; Trident/4.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; InfoPath.2; OfficeLiveConnector.1.3; OfficeLivePatch.0.0; MS-RTC LM 8; Zune 4.0)&#39;,        &#39;Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 6.1; WOW64; Trident/4.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; InfoPath.2; MS-RTC LM 8)&#39;,    ]headers = {        &#39;User-Agent&#39;: random.choice(user_agent_list)    }# 随机生成一个11位的cookienum = &#39;&#39;.join(random.sample(string.digits + string.ascii_letters, 11))cookie = {&#39;bid&#39;: num, &#39;ll&#39;: &#39;&quot;108296&quot;&#39;}# 代理def get_proxy():    return requests.get(&quot;http://127.0.0.1:5010/get/&quot;).textdef delete_proxy(proxy):    requests.get(&quot;http://127.0.0.1:5010/delete/?proxy={}&quot;.format(proxy))def get_page(url):    while 1:        try:            proxies = {&#39;http&#39;:&#39;http://&#39;+get_proxy()}            print(proxies)            r = requests.get(url, headers = headers, proxies = proxies, cookies = cookie)            return json.loads(r.text)[&#39;data&#39;]        except Exception:            delete_proxy(proxy)def connect_sql():    global conn,cursor    conn = pymysql.connect(        host=&#39;127.0.0.1&#39;,        port=3306,        user=&#39;root&#39;,        password=&#39;&#39;,        database=&#39;douban&#39;,        charset=&#39;utf8&#39;)    # 获取一个光标    cursor = conn.cursor()def save_infos(data):    for row in data:        try:            sql = &#39;insert into movie(directors,rate,cover_x,star,title,url,casts,cover,id,cover_y) values(&quot;%s&quot;,&quot;%s&quot;,&quot;%s&quot;,&quot;%s&quot;,&quot;%s&quot;,&quot;%s&quot;,&quot;%s&quot;,&quot;%s&quot;,&quot;%s&quot;,&quot;%s&quot;)&#39;%(row[&#39;directors&#39;],row[&#39;rate&#39;],row[&#39;cover_x&#39;],row[&#39;star&#39;],row[&#39;title&#39;],row[&#39;url&#39;],row[&#39;casts&#39;],row[&#39;cover&#39;],row[&#39;id&#39;],row[&#39;cover_y&#39;])            cursor.execute(sql)            conn.commit()        except Exception:            conn.rollback()# 获取URLdef get_url():    type_ = [&#39;剧情&#39;,&#39;喜剧&#39;,&#39;动作&#39;,&#39;爱情&#39;,&#39;科幻&#39;,&#39;动画&#39;,&#39;悬疑&#39;,&#39;惊悚&#39;,&#39;恐怖&#39;,&#39;犯罪&#39;,&#39;同性&#39;,&#39;音乐&#39;,&#39;歌舞&#39;,&#39;传记&#39;,&#39;历史&#39;,&#39;战争&#39;,&#39;西部&#39;,&#39;奇幻&#39;,&#39;冒险&#39;,&#39;灾难&#39;,&#39;武侠&#39;,&#39;情色&#39;]    region = [&#39;中国大陆&#39;,&#39;美国&#39;,&#39;中国香港&#39;,&#39;中国台湾&#39;,&#39;日本&#39;,&#39;韩国&#39;,&#39;英国&#39;,&#39;法国&#39;,&#39;德国&#39;,&#39;意大利&#39;,&#39;西班牙&#39;,&#39;印度&#39;,&#39;泰国&#39;,&#39;俄罗斯&#39;,&#39;伊朗&#39;,&#39;加拿大&#39;,&#39;澳大利亚&#39;,&#39;爱尔兰&#39;,&#39;瑞典&#39;,&#39;巴西&#39;,&#39;丹麦&#39;]    year1 = [str(i) for i in range(1960,2016,5)]    year2 = [str(i) for i in range(1964,2020,5)]    t_url = [&#39;&amp;genres=&#39; + parse.quote(i) for i in type_]    r_url = [t_url[j] + &#39;&amp;countries=&#39; + parse.quote(i) for j in range(0,len(t_url)) for i in region]    y_url = [r_url[j] + &#39;&amp;year_range=&#39; + year1[i] + &#39;,&#39; + year2[i] for j in range(0,len(r_url)) for i in range(0,12)]    return y_url# main函数def main():    connect_sql()    t = time.time()    base_url = &#39;https://movie.douban.com/j/new_search_subjects?sort=T&amp;range=7,10&amp;tags=电影&amp;start=&#39;    #开始循环      urls = get_url()    # 外层循环：遍历所有标签    for url in urls:        n = 0        # 内层循环：每次循环增加页码，如果返回数据为空，则跳出循环，说明已经拿不到更多数据了。        while 1:            u = base_url + str(n) + url            data = get_page(u)            if data == []:                print(u)                break            print(data[0])            save_infos(data)            n += 20    conn.close()    print(time.time()-t)if __name__ == &#39;__main__&#39;:    main()</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;同步版本的速度要慢一点，但是优点在于：当到达某一页面返回值为空时，不再请求后面的页面，减少可请求次数。&lt;/p&gt;
    
    </summary>
    
      <category term="爬虫" scheme="http://xiaoguiwk.xyz/categories/%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="豆瓣" scheme="http://xiaoguiwk.xyz/tags/%E8%B1%86%E7%93%A3/"/>
    
      <category term="爬虫" scheme="http://xiaoguiwk.xyz/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>豆瓣电影爬虫（四）完整代码</title>
    <link href="http://xiaoguiwk.xyz/2019/08/22/%E8%B1%86%E7%93%A3%E7%94%B5%E5%BD%B1%E7%88%AC%E8%99%AB%EF%BC%88%E5%9B%9B%EF%BC%89%E5%AE%8C%E6%95%B4%E4%BB%A3%E7%A0%81/"/>
    <id>http://xiaoguiwk.xyz/2019/08/22/豆瓣电影爬虫（四）完整代码/</id>
    <published>2019-08-22T09:38:22.000Z</published>
    <updated>2019-08-22T11:28:49.667Z</updated>
    
    <content type="html"><![CDATA[<p>这里是豆瓣电影异步爬虫的完整代码。</p><a id="more"></a><pre><code class="python">from urllib import parseimport jsonimport aiohttpimport asyncioimport requestsimport timeimport pymysqlinfo = []conn = Nonecursor = None#这里的cookies必须设置headers = {            &#39;Referer&#39;: &#39;https://space.bilibili.com/38690046&#39;,            &#39;User-Agent&#39;: &#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.142 Safari/537.36&#39;            }# 代理def get_proxy():    return requests.get(&quot;http://127.0.0.1:5010/get/&quot;).textdef delete_proxy(proxy):    requests.get(&quot;http://127.0.0.1:5010/delete/?proxy={}&quot;.format(proxy))# fetch函数async def fetch(session, url):    while 1:        try:            proxy = &#39;http://&#39;+get_proxy()            print(proxy)            async with session.get(url,proxy = proxy) as response:                return await response.read()        except Exception:            delete_proxy(proxy)# get函数async def get_html(url,semaphore):    async with semaphore:        async with aiohttp.ClientSession(headers=headers) as session:            html = await fetch(session, url)            await parse_html(html)            await asyncio.sleep(1)# parse函数async def parse_html(r):    try:        data = json.loads(r)[&#39;data&#39;]        print(data[0])        info + = data    except IndexError:        pass# 数据存储def connect_sql():    global conn,cursor    conn = pymysql.connect(        host=&#39;127.0.0.1&#39;,        port=3306,        user=&#39;root&#39;,        password=&#39;&#39;,        database=&#39;douban&#39;,        charset=&#39;utf8&#39;)    # 获取一个光标    cursor = conn.cursor()def save_infos():    global info, conn, cursor    for row in info:        try:            sql = &#39;insert into movie(directors,rate,cover_x,star,title,url,casts,cover,id,cover_y) values(&quot;%s&quot;,&quot;%s&quot;,&quot;%s&quot;,&quot;%s&quot;,&quot;%s&quot;,&quot;%s&quot;,&quot;%s&quot;,&quot;%s&quot;,&quot;s&quot;,&quot;s&quot;)&#39;%(row[&#39;directors&#39;],row[&#39;rate&#39;],row[&#39;cover_x&#39;],row[&#39;star&#39;],row[&#39;title&#39;],row[&#39;url&#39;],row[&#39;casts&#39;],row[&#39;cover&#39;],row[&#39;id&#39;],row[&#39;cover_y&#39;])            cursor.execute(sql)            conn.commit()        except Exception:            conn.rollback()# 获取URLdef get_url():    # type包括了全部类型    type = [&#39;剧情&#39;,&#39;喜剧&#39;,&#39;动作&#39;,&#39;爱情&#39;,&#39;科幻&#39;,&#39;动画&#39;,&#39;悬疑&#39;,&#39;惊悚&#39;,&#39;恐怖&#39;,&#39;犯罪&#39;,&#39;同性&#39;,&#39;音乐&#39;,&#39;歌舞&#39;,&#39;传记&#39;,&#39;历史&#39;,&#39;战争&#39;,&#39;西部&#39;,&#39;奇幻&#39;,&#39;冒险&#39;,&#39;灾难&#39;,&#39;武侠&#39;,&#39;情色&#39;]    # region包括了全部地区    region = [&#39;中国大陆&#39;,&#39;美国&#39;,&#39;中国香港&#39;,&#39;中国台湾&#39;,&#39;日本&#39;,&#39;韩国&#39;,&#39;英国&#39;,&#39;法国&#39;,&#39;德国&#39;,&#39;意大利&#39;,&#39;西班牙&#39;,&#39;印度&#39;,&#39;泰国&#39;,&#39;俄罗斯&#39;,&#39;伊朗&#39;,&#39;加拿大&#39;,&#39;澳大利亚&#39;,&#39;爱尔兰&#39;,&#39;瑞典&#39;,&#39;巴西&#39;,&#39;丹麦&#39;]    # 年份设置    year1 = [str(i) for i in range(1960,2016,5)]    year2 = [str(i) for i in range(1964,2020,5)]    base_url = [&#39;https://movie.douban.com/j/new_search_subjects?sort=T&amp;range=7,10&amp;tags=电影&amp;start=&#39;+str(i)+&#39;&amp;genres=&#39; for i in range(0,1000,20)]    # 先拼接type,得到一个长度为22x50的列表    t_url = [base_url[j] + parse.quote(i) for j in range(0,len(base_url)) for i in type]    r_url = [t_url[j] + &#39;&amp;countries=&#39; + parse.quote(i) for j in range(0,len(t_url)) for i in region]    # 拼接年份    y_url = [r_url[j] + &#39;&amp;year_range=&#39; + year1[i] + &#39;,&#39; + year2[i] for j in range(0,462) for i in range(0,12)]    return y_url# main函数def main():    connect_sql()    t = time.time()    urls = get_url()    print(urls[1])    semaphore = asyncio.Semaphore(200)    loop = asyncio.get_event_loop()    tasks = [asyncio.ensure_future(get_html(url,semaphore)) for url in urls]    tasks = asyncio.gather(*tasks)    loop.run_until_complete(tasks)    save_infos()    conn.close()    print(time.time()-t)if __name__ == &#39;__main__&#39;:    main()</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这里是豆瓣电影异步爬虫的完整代码。&lt;/p&gt;
    
    </summary>
    
      <category term="爬虫" scheme="http://xiaoguiwk.xyz/categories/%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="豆瓣" scheme="http://xiaoguiwk.xyz/tags/%E8%B1%86%E7%93%A3/"/>
    
      <category term="爬虫" scheme="http://xiaoguiwk.xyz/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>豆瓣电影爬虫（三）数据库设计</title>
    <link href="http://xiaoguiwk.xyz/2019/08/22/%E8%B1%86%E7%93%A3%E7%94%B5%E5%BD%B1%E7%88%AC%E8%99%AB%EF%BC%88%E4%B8%89%EF%BC%89%E6%95%B0%E6%8D%AE%E5%BA%93%E8%AE%BE%E8%AE%A1/"/>
    <id>http://xiaoguiwk.xyz/2019/08/22/豆瓣电影爬虫（三）数据库设计/</id>
    <published>2019-08-22T09:17:41.000Z</published>
    <updated>2019-08-22T11:32:36.354Z</updated>
    
    <content type="html"><![CDATA[<p>将爬虫分析到的10个字断存储在数据库中。</p><a id="more"></a><pre><code class="sql">create database douban;create table movie(    id int primary key not null,    title varchar(200) not null,    directors varchar(100),    star int,    rate int,    casts varchar(200),    cover varchar(100),    cover_x varchar(100),    cover_y varchar(100),    url varchar(200))default charset =utf8</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;将爬虫分析到的10个字断存储在数据库中。&lt;/p&gt;
    
    </summary>
    
      <category term="爬虫" scheme="http://xiaoguiwk.xyz/categories/%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="豆瓣" scheme="http://xiaoguiwk.xyz/tags/%E8%B1%86%E7%93%A3/"/>
    
      <category term="爬虫" scheme="http://xiaoguiwk.xyz/tags/%E7%88%AC%E8%99%AB/"/>
    
      <category term="数据库" scheme="http://xiaoguiwk.xyz/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>豆瓣电影爬虫（二）异步</title>
    <link href="http://xiaoguiwk.xyz/2019/08/22/%E8%B1%86%E7%93%A3%E7%94%B5%E5%BD%B1%E7%88%AC%E8%99%AB%EF%BC%88%E4%BA%8C%EF%BC%89%E5%BC%82%E6%AD%A5/"/>
    <id>http://xiaoguiwk.xyz/2019/08/22/豆瓣电影爬虫（二）异步/</id>
    <published>2019-08-22T08:21:40.000Z</published>
    <updated>2019-08-22T11:25:59.840Z</updated>
    
    <content type="html"><![CDATA[<p>异步爬虫的完整分析和思路。</p><a id="more"></a><h1 id="框架"><a href="#框架" class="headerlink" title="框架"></a>框架</h1><p>其实没有框架，这里套用之前爬哔哩哔哩的异步代码，参考GitHub代码：<a href="https://github.com/Gjh9508/asyncbili/blob/master/users.py" target="_blank" rel="noopener">https://github.com/Gjh9508/asyncbili/blob/master/users.py</a></p><h2 id="代理设置"><a href="#代理设置" class="headerlink" title="代理设置"></a>代理设置</h2><pre><code>def get_proxy():    return requests.get(&quot;http://127.0.0.1:5010/get/&quot;).textdef delete_proxy(proxy):    requests.get(&quot;http://127.0.0.1:5010/delete/?proxy={}&quot;.format(proxy))</code></pre><h2 id="fetch函数"><a href="#fetch函数" class="headerlink" title="fetch函数"></a>fetch函数</h2><pre><code>async def fetch(session, url):    while 1:        try:            proxy = &#39;http://&#39;+get_proxy()            print(proxy)            async with session.get(url,proxy = proxy) as response:                return await response.read()        except Exception:            delete_proxy(proxy)</code></pre><h2 id="parse函数"><a href="#parse函数" class="headerlink" title="parse函数"></a>parse函数</h2><p><font color="red">这里的parse函数和哔哩哔哩爬虫有点不一样，这里的json一次可以获取20个数据。也就是说返回值是一个列表，因此将info.append()换成info+=的形式。</font><br><font color="green">这里的异常处理主要是处理当请求的返回data为空时需要做的处理。</font></p><pre><code>async def parse_html(r):    try:        data = json.loads(r)[&#39;data&#39;]        print(data[0])        info + = data    except IndexError:        pass</code></pre><h2 id="存储设置"><a href="#存储设置" class="headerlink" title="存储设置"></a>存储设置</h2><ul><li>一共10个参数</li></ul><pre><code>def connect_sql():    global conn,cursor    conn = pymysql.connect(        host=&#39;127.0.0.1&#39;,        port=3306,        user=&#39;root&#39;,        password=&#39;&#39;,        database=&#39;douban&#39;,        charset=&#39;utf8&#39;)    # 获取一个光标    cursor = conn.cursor()def save_infos():    global info, conn, cursor    for row in info:        try:            sql = &#39;insert into movie(directors,rate,cover_x,star,title,url,casts,cover,id,cover_y) values(&quot;%s&quot;,&quot;%s&quot;,&quot;%s&quot;,&quot;%s&quot;,&quot;%s&quot;,&quot;%s&quot;,&quot;%s&quot;,&quot;%s&quot;,&quot;s&quot;,&quot;s&quot;)&#39;%(row[&#39;directors&#39;],row[&#39;rate&#39;],row[&#39;cover_x&#39;],row[&#39;star&#39;],row[&#39;title&#39;],row[&#39;url&#39;],row[&#39;casts&#39;],row[&#39;cover&#39;],row[&#39;id&#39;],row[&#39;cover_y&#39;])            cursor.execute(sql)            conn.commit()        except Exception:            conn.rollback()    info = []</code></pre><h2 id="get-url-函数"><a href="#get-url-函数" class="headerlink" title="get_url()函数"></a>get_url()函数</h2><p>这个函数设置成同步即可</p><p><font color="red">到这里碰到了一个致命的问题。</font></p><ul><li><p>如何判断每一个类别有多少页？</p></li><li><p>如果不知道每个类别有多页，那怎么生成URL？</p></li><li><p>经过分析，五年内一个标签不能超过1000条数据。</p></li></ul><pre><code>def get_url():    from urllib import parse    # type包括了全部类型    type = [&#39;剧情&#39;,&#39;喜剧&#39;,&#39;动作&#39;,&#39;爱情&#39;,&#39;科幻&#39;,&#39;动画&#39;,&#39;悬疑&#39;,&#39;惊悚&#39;,&#39;恐怖&#39;,&#39;犯罪&#39;,&#39;同性&#39;,&#39;音乐&#39;,&#39;歌舞&#39;,&#39;传记&#39;,&#39;历史&#39;,&#39;战争&#39;,&#39;西部&#39;,&#39;奇幻&#39;,&#39;冒险&#39;,&#39;灾难&#39;,&#39;武侠&#39;,&#39;情色&#39;]    # region包括了全部地区    region = [&#39;中国大陆&#39;,&#39;美国&#39;,&#39;中国香港&#39;,&#39;中国台湾&#39;,&#39;日本&#39;,&#39;韩国&#39;,&#39;英国&#39;,&#39;法国&#39;,&#39;德国&#39;,&#39;意大利&#39;,&#39;西班牙&#39;,&#39;印度&#39;,&#39;泰国&#39;,&#39;俄罗斯&#39;,&#39;伊朗&#39;,&#39;加拿大&#39;,&#39;澳大利亚&#39;,&#39;爱尔兰&#39;,&#39;瑞典&#39;,&#39;巴西&#39;,&#39;丹麦&#39;]    # 年份设置    year1 = [str(i) for i in range(1960,2016,5)]    year2 = [str(i) for i in range(1964,2020,5)]    base_url = [&#39;https://movie.douban.com/j/new_search_subjects?sort=T&amp;range=7,10&amp;tags=电影&amp;start=&#39;+str(i)+&#39;&amp;genres=&#39; for i in range(0,1000,20)]    # 先拼接type,得到一个长度为22x50的列表    t_url = [base_url[j] + parse.quote(i) for j in range(0,len(base_url)) for i in type]    r_url = [t_url[j] + &#39;&amp;countries=&#39; + parse.quote(i) for j in range(0,len(t_url)) for i in region]    # 拼接年份    y_url = [r_url[j] + &#39;&amp;year_range=&#39; + year1[i] + &#39;,&#39; + year2[i] for j in range(0,462) for i in range(0,12)]    return y_url</code></pre><ul><li>到这里我们解决了翻页的问题。</li></ul><h2 id="main函数"><a href="#main函数" class="headerlink" title="main函数"></a>main函数</h2><pre><code>def main():    connect_sql()    t = time.time()    urls = get_url()    print(urls[1])    semaphore = asyncio.Semaphore(200)    loop = asyncio.get_event_loop()    tasks = [asyncio.ensure_future(get_html(url,semaphore)) for url in urls]    tasks = asyncio.gather(*tasks)    loop.run_until_complete(tasks)    save_infos()    conn.close()    print(time.time()-t)</code></pre><h2 id="最后运行即可"><a href="#最后运行即可" class="headerlink" title="最后运行即可"></a>最后运行即可</h2><pre><code>if __name__ == &#39;__main__&#39;:    main()</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;异步爬虫的完整分析和思路。&lt;/p&gt;
    
    </summary>
    
      <category term="爬虫" scheme="http://xiaoguiwk.xyz/categories/%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="豆瓣" scheme="http://xiaoguiwk.xyz/tags/%E8%B1%86%E7%93%A3/"/>
    
      <category term="爬虫" scheme="http://xiaoguiwk.xyz/tags/%E7%88%AC%E8%99%AB/"/>
    
      <category term="异步" scheme="http://xiaoguiwk.xyz/tags/%E5%BC%82%E6%AD%A5/"/>
    
  </entry>
  
  <entry>
    <title>豆瓣电影爬虫（一）思路</title>
    <link href="http://xiaoguiwk.xyz/2019/08/22/%E8%B1%86%E7%93%A3%E7%94%B5%E5%BD%B1%E7%88%AC%E8%99%AB%EF%BC%88%E4%B8%80%EF%BC%89%E6%80%9D%E8%B7%AF/"/>
    <id>http://xiaoguiwk.xyz/2019/08/22/豆瓣电影爬虫（一）思路/</id>
    <published>2019-08-22T06:05:51.000Z</published>
    <updated>2019-08-22T11:25:52.736Z</updated>
    
    <content type="html"><![CDATA[<p>豆瓣收录的电影数量非常大，如何尽可能多的拿到电影的编号是最大的难题。本文的思路仅仅是通过tags分类得到更多的编号，并没有拿到所有编号的解决方案。</p><a id="more"></a><h1 id="目标网站"><a href="#目标网站" class="headerlink" title="目标网站"></a>目标网站</h1><p><a href="https://movie.douban.com/" target="_blank" rel="noopener">豆瓣电影</a><br><a href="https://movie.douban.com/tag/#/" target="_blank" rel="noopener">电影分类</a></p><h1 id="页面分析"><a href="#页面分析" class="headerlink" title="页面分析"></a>页面分析</h1><ul><li><p>关于如何分析异步加载、获取json数据、判断请求方式、请求头需要携带的参数等等略过。</p></li><li><p>打开电影分类的网页得到的页面如下图：</p></li></ul><div align="center"><img src="/images/豆瓣电影爬虫（一）思路/1.jpg" title="豆瓣电影分类"></div><blockquote><p>通过观察这个页面，可以发现这个页面基本上已经做到我想要的东西了。。。但是话说回来，我要做的是：把这个页面爬下来，再爬下来详情页面，存在数据库的两张表中，我就可以开心的练习数据库了。</p></blockquote><ul><li>分析：</li></ul><ol><li>如果直接上手爬的话，不知道能拿到多少数据。之前在分析<a href="books.douban.com">豆瓣阅读</a>的时候就发现，在某一个tag下只能拿到50页的数据，也就是一个tag只能访问到1000本书。尽管是通过浏览器访问也是拿不到的。所以直接这样爬肯定有所限制。</li><li>豆瓣默认的排序大概加入了评价人数、分数、年份等等因素。我想做的是抛开一切因素，拿到这些数据。</li><li>标签分析：给出的大分类有形式、类型、地区、年代、特色五个分类。由于本次爬虫目标是电影，因此形式这里必须是选择电影了。而其他四个分类，各自都有许多标签，那么问题就来了：</li></ol><blockquote><p>如果我每一次请求都带上一个标签，就可以细化每次的请求，是不是可以尽可能拿到更多的数据？</p></blockquote><h1 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h1><ul><li>tags设计</li></ul><pre><code># tags# type包括了全部类型type = [&#39;剧情&#39;,&#39;喜剧&#39;,&#39;动作&#39;,&#39;爱情&#39;,&#39;科幻&#39;,&#39;动画&#39;,&#39;悬疑&#39;,&#39;惊悚&#39;,&#39;恐怖&#39;,&#39;犯罪&#39;,&#39;同性&#39;,&#39;音乐&#39;,&#39;歌舞&#39;,&#39;传记&#39;,&#39;历史&#39;,&#39;战争&#39;,&#39;西部&#39;,&#39;奇幻&#39;,&#39;冒险&#39;,&#39;灾难&#39;,&#39;武侠&#39;,&#39;情色&#39;]# region包括了全部地区region = [&#39;中国大陆&#39;,&#39;美国&#39;,&#39;中国香港&#39;,&#39;中国台湾&#39;,&#39;日本&#39;,&#39;韩国&#39;,&#39;英国&#39;,&#39;法国&#39;,&#39;德国&#39;,&#39;意大利&#39;,&#39;西班牙&#39;,&#39;印度&#39;,&#39;泰国&#39;,&#39;俄罗斯&#39;,&#39;伊朗&#39;,&#39;加拿大&#39;,&#39;澳大利亚&#39;,&#39;爱尔兰&#39;,&#39;瑞典&#39;,&#39;巴西&#39;,&#39;丹麦&#39;]# years包括了全部年代（通过80年代等可以判断这个字段接受的是字符串）years = [&#39;2019&#39;,&#39;2018&#39;,&#39;2010年代&#39;,&#39;2000年代&#39;,&#39;90年代&#39;,&#39;80年代&#39;,&#39;70年代&#39;,&#39;60年代&#39;,&#39;更早&#39;]# characteristics 包括了全部特色characteristics = [&#39;经典&#39;,&#39;青春&#39;,&#39;文艺&#39;,&#39;搞笑&#39;,&#39;励志&#39;,&#39;魔幻&#39;,&#39;感人&#39;,&#39;女性&#39;,&#39;黑帮&#39;]</code></pre><ul><li>在设计tags的过程中我发现，在年代的列表中，只有2019年和2018年被区别对待了。原因很容易就能看出来，这两年比较接近现在。那如果想要重点对待某一年或者某一段时间，只需要自己再加上就好了。在这里可以看一下请求的链接形式：</li></ul><pre><code>url = &#39;https://movie.douban.com/j/new_search_subjects?sort=U&amp;range=0,10&amp;tags=经典&amp;start=0&amp;countries=中国大陆&amp;year_range=2019,2019&#39;</code></pre><p>请求的URL包括一下几个参数：</p><ol><li>sort=U: 排序方式，可以选择以下几个：</li></ol><table><thead><tr><th align="center">符号</th><th align="center">U</th><th align="center">S</th><th align="center">T</th><th align="center">R</th></tr></thead><tbody><tr><td align="center">含义</td><td align="center">近期热门</td><td align="center">评分最高</td><td align="center">标记最多</td><td align="center">最新上映</td></tr></tbody></table><ol start="2"><li>range=0,10 : 评分筛选，可以选择筛选的电影的评分区间，这里一般设置为7，10就可以筛选出来大部分好电影。有一个问题在于<strong>这里的0，10的数据类型是什么样的</strong>；</li><li>tags=中国大陆： 这里的tags指的是前面设置的type；</li><li>start=0: 即为偏移量。需要注意的地方是请求链接里不能设置步长，每次请求默认返回20个；</li><li>countries: 地区，不多解释；</li><li>year_range=2019,2019: 这个字段很有意思。我们发出的请求是2019，但是通过URL拼接就变成了一个元组一样的东西，表示了年份区间。因此在设置年份的时候可以有以下几个方式：</li></ol><pre><code># 年份设置# 默认设置方式years = [&#39;2019&#39;,&#39;2018&#39;,&#39;2010年代&#39;,&#39;2000年代&#39;,&#39;90年代&#39;,&#39;80年代&#39;,&#39;70年代&#39;,&#39;60年代&#39;,&#39;更早&#39;]# 生成一个从1960年到2019年的列表并转成字符串，加上一个更早的年份区间years = [str(i) for i in range(1960,2020)] + [&#39;更早&#39;]# 将需要区别对待的年份单独列出,这里区别对待了90年代所有的年份years = [&#39;2019&#39;,&#39;2018&#39;,&#39;2010年代&#39;,&#39;2000年代&#39;,&#39;80年代&#39;,&#39;70年代&#39;,&#39;60年代&#39;,&#39;更早&#39;] + [str(i) for i in range(1990,1999)]</code></pre><p><font color="red" size="5">但是问题出现了：我们虽然在很努力的设置年份，但是忘了一件事情。在构造URL时，接收的参数是两个而不是一个，也就是说，years这个参数，需要两个列表来完成。</font></p><p><font color="blue" size="5">也就是说，要写成如下形式：</font></p><pre><code># 先设置前year1 = [&#39;2019&#39;,&#39;2018&#39;,&#39;2014&#39;,&#39;2010&#39;,&#39;2005&#39;,&#39;2000&#39;,&#39;1995&#39;,&#39;1990&#39;...]# 再设置后year2 = [&#39;2019&#39;,&#39;2018&#39;,&#39;2017&#39;,&#39;2013&#39;,&#39;2009&#39;,&#39;2004&#39;,&#39;1999&#39;,&#39;1994&#39;...]</code></pre><p>也就是说，两个需要互相对应，组成年份参数。<font color="red">比如我现在不需要特殊对待这两年，我只需要每隔五年用作一个区间就好了，那就可以用列表生成式。</font></p><pre><code>year1 = [str(i) for i in range(1960,2016,5)]year2 = [str(i) for i in range(1964,2020,5)]</code></pre><p>生成的结果如下所示：</p><blockquote><div align="center">[1960, 1965, 1970, 1975, 1980, 1985, 1990, 1995, 2000, 2005, 2010, 2015]</div><div align="center">[1964, 1969, 1974, 1979, 1984, 1989, 1994, 1999, 2004, 2009, 2014, 2019]</div></blockquote><p><strong>正好一共12个区间。</strong></p><h1 id="好了现在可以生成URL了"><a href="#好了现在可以生成URL了" class="headerlink" title="好了现在可以生成URL了"></a>好了现在可以生成URL了</h1><p>现在有下面几个参数：</p><ol><li>sort=T,即按照标记数量排序；</li><li>range=7,10 即筛选7到10分之间的电影；</li><li>tags, 按照type列表遍历；</li><li>start=0, 当然是从0开始，但需要捕捉异常；</li><li>countries 列表遍历；</li><li>year1,year2按照两个年份列表遍历。</li></ol><h1 id="问题来了——这个问题看上去像是个3层嵌套循环"><a href="#问题来了——这个问题看上去像是个3层嵌套循环" class="headerlink" title="问题来了——这个问题看上去像是个3层嵌套循环"></a>问题来了——这个问题看上去像是个3层嵌套循环</h1><ul><li>现在先用列表生成式：</li></ul><pre><code># type包括了全部类型type = [&#39;剧情&#39;,&#39;喜剧&#39;,&#39;动作&#39;,&#39;爱情&#39;,&#39;科幻&#39;,&#39;动画&#39;,&#39;悬疑&#39;,&#39;惊悚&#39;,&#39;恐怖&#39;,&#39;犯罪&#39;,&#39;同性&#39;,&#39;音乐&#39;,&#39;歌舞&#39;,&#39;传记&#39;,&#39;历史&#39;,&#39;战争&#39;,&#39;西部&#39;,&#39;奇幻&#39;,&#39;冒险&#39;,&#39;灾难&#39;,&#39;武侠&#39;,&#39;情色&#39;]# region包括了全部地区region = [&#39;中国大陆&#39;,&#39;美国&#39;,&#39;中国香港&#39;,&#39;中国台湾&#39;,&#39;日本&#39;,&#39;韩国&#39;,&#39;英国&#39;,&#39;法国&#39;,&#39;德国&#39;,&#39;意大利&#39;,&#39;西班牙&#39;,&#39;印度&#39;,&#39;泰国&#39;,&#39;俄罗斯&#39;,&#39;伊朗&#39;,&#39;加拿大&#39;,&#39;澳大利亚&#39;,&#39;爱尔兰&#39;,&#39;瑞典&#39;,&#39;巴西&#39;,&#39;丹麦&#39;]# 年份设置year1 = [str(i) for i in range(1960,2016,5)]year2 = [str(i) for i in range(1964,2020,5)]base_url = &#39;https://movie.douban.com/j/new_search_subjects?sort=T&amp;range=7,10&amp;tags=电影&amp;start=0&amp;genres=&#39;# 先拼接type,得到一个长度为22的列表t_url = [base_url + i for i in type]# 拼接regionr_url = [t_url + &#39;&amp;countries=&#39; + i for i in region]</code></pre><p><font color="red">好了还没开始就报错了，我像是个傻子.</font></p><p>ps:<a href="https://www.cnblogs.com/vinnson/p/10845137.html" target="_blank" rel="noopener">字体设置在这里</a></p><blockquote><p>这个问题更像是一个排列组合问题但不是。既然用了python那就要想方设法避免循环，更要避免循环嵌套。否则要慢的骂街。</p></blockquote><p>这个问题，稍微计算一下：<br>拼接完type的长度是22；<br>那么拼接完地区，长度应该是22xlen(region)=22x21=462;<br>接下来拼接年份，长度变成462x12 = 5544;</p><ul><li>那么拼接就变成了下面这个样子</li></ul><pre><code># 拼接regionr_url = [t_url[j] + &#39;&amp;countries=&#39; + i for j in range(0,len(t_url)) for i in region]# 拼接年份y_url = [r_url[j] + &#39;&amp;year_range=&#39; + year1[i] + &#39;,&#39; + year2[i] for j in range(0,462) for i in range(0,12)]</code></pre><ul><li>出现的新的问题，URL中含有中文改怎么解决，现在拼接成功的URL是下面这个样子：</li></ul><blockquote><p><a href="https://movie.douban.com/j/new_search_subjects?sort=T&amp;range=7,10&amp;tags=\xe7\x94\xb5\xe5\xbd\xb1&amp;start=0&amp;genres=\xe5\x89\xa7\xe6\x83\x85&amp;countries=\xe4\xb8\xad\xe5\x9b\xbd\xe5\xa4\xa7\xe9\x99\x86&amp;year_range=1960,1964" target="_blank" rel="noopener">https://movie.douban.com/j/new_search_subjects?sort=T&amp;range=7,10&amp;tags=\xe7\x94\xb5\xe5\xbd\xb1&amp;start=0&amp;genres=\xe5\x89\xa7\xe6\x83\x85&amp;countries=\xe4\xb8\xad\xe5\x9b\xbd\xe5\xa4\xa7\xe9\x99\x86&amp;year_range=1960,1964</a></p></blockquote><p><font color="red">需要用到下面这种方法：</font></p><pre><code>from urllib import parse# type包括了全部类型type = [&#39;剧情&#39;,&#39;喜剧&#39;,&#39;动作&#39;,&#39;爱情&#39;,&#39;科幻&#39;,&#39;动画&#39;,&#39;悬疑&#39;,&#39;惊悚&#39;,&#39;恐怖&#39;,&#39;犯罪&#39;,&#39;同性&#39;,&#39;音乐&#39;,&#39;歌舞&#39;,&#39;传记&#39;,&#39;历史&#39;,&#39;战争&#39;,&#39;西部&#39;,&#39;奇幻&#39;,&#39;冒险&#39;,&#39;灾难&#39;,&#39;武侠&#39;,&#39;情色&#39;]# region包括了全部地区region = [&#39;中国大陆&#39;,&#39;美国&#39;,&#39;中国香港&#39;,&#39;中国台湾&#39;,&#39;日本&#39;,&#39;韩国&#39;,&#39;英国&#39;,&#39;法国&#39;,&#39;德国&#39;,&#39;意大利&#39;,&#39;西班牙&#39;,&#39;印度&#39;,&#39;泰国&#39;,&#39;俄罗斯&#39;,&#39;伊朗&#39;,&#39;加拿大&#39;,&#39;澳大利亚&#39;,&#39;爱尔兰&#39;,&#39;瑞典&#39;,&#39;巴西&#39;,&#39;丹麦&#39;]# 年份设置year1 = [str(i) for i in range(1960,2016,5)]year2 = [str(i) for i in range(1964,2020,5)]base_url = &#39;https://movie.douban.com/j/new_search_subjects?sort=T&amp;range=7,10&amp;tags=电影&amp;start=0&amp;genres=&#39;# 先拼接type,得到一个长度为22的列表t_url = [base_url + parse.quote(i) for i in type]r_url = [t_url[j] + &#39;&amp;countries=&#39; + parse.quote(i) for j in range(0,len(t_url)) for i in region]# 拼接年份y_url = [r_url[j] + &#39;&amp;year_range=&#39; + year1[i] + &#39;,&#39; + year2[i] for j in range(0,462) for i in range(0,12)]</code></pre><p>测试</p><pre><code>$ python3&gt;&gt;&gt; from .....&gt;&gt;&gt; y_url[0]&#39;https://movie.douban.com/j/new_search_subjects?sort=T&amp;range=7,10&amp;tags=电影&amp;start=0&amp;genres=%E5%89%A7%E6%83%85&amp;countries=%E4%B8%AD%E5%9B%BD%E5%A4%A7%E9%99%86&amp;year_range=1960,1964&#39;</code></pre><h1 id="BINGO"><a href="#BINGO" class="headerlink" title="BINGO!"></a>BINGO!</h1><h2 id="到此为止生成了5544个URL，下一步就是取了。"><a href="#到此为止生成了5544个URL，下一步就是取了。" class="headerlink" title="到此为止生成了5544个URL，下一步就是取了。"></a>到此为止生成了5544个URL，下一步就是取了。</h2>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;豆瓣收录的电影数量非常大，如何尽可能多的拿到电影的编号是最大的难题。本文的思路仅仅是通过tags分类得到更多的编号，并没有拿到所有编号的解决方案。&lt;/p&gt;
    
    </summary>
    
      <category term="爬虫" scheme="http://xiaoguiwk.xyz/categories/%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="豆瓣" scheme="http://xiaoguiwk.xyz/tags/%E8%B1%86%E7%93%A3/"/>
    
      <category term="爬虫" scheme="http://xiaoguiwk.xyz/tags/%E7%88%AC%E8%99%AB/"/>
    
      <category term="异步" scheme="http://xiaoguiwk.xyz/tags/%E5%BC%82%E6%AD%A5/"/>
    
      <category term="Markdown字体" scheme="http://xiaoguiwk.xyz/tags/Markdown%E5%AD%97%E4%BD%93/"/>
    
      <category term="Markdown表格" scheme="http://xiaoguiwk.xyz/tags/Markdown%E8%A1%A8%E6%A0%BC/"/>
    
      <category term="Markdown图片" scheme="http://xiaoguiwk.xyz/tags/Markdown%E5%9B%BE%E7%89%87/"/>
    
  </entry>
  
  <entry>
    <title>服务器笔记</title>
    <link href="http://xiaoguiwk.xyz/2019/08/20/%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%AC%94%E8%AE%B0/"/>
    <id>http://xiaoguiwk.xyz/2019/08/20/服务器笔记/</id>
    <published>2019-08-20T05:44:45.000Z</published>
    <updated>2019-08-20T15:59:45.403Z</updated>
    
    <content type="html"><![CDATA[<p>主要记录Linux系统在使用过程中碰到的问题及一些解决办法。包括各个有可能的方面。</p><a id="more"></a><h1 id="连接ssh"><a href="#连接ssh" class="headerlink" title="连接ssh"></a>连接ssh</h1><ul><li>输入<pre><code>ssh -p 22 root@101.200.86.233</code></pre>它会提示你输入密码,输入正确的密码之后,你就发现已经登陆成功了.(22: 端口号 root: 用户名)<h1 id="后台运行"><a href="#后台运行" class="headerlink" title="后台运行"></a>后台运行</h1></li><li>Nohup python3 main.py &amp;，输入命令后回车退出 </li><li>查看输出内容：tail -f nohup.out </li><li>关闭：ps -aux | grep “users.py”查看tid，kill tid</li><li>查看后台程序：jobs -l（这是L）<h1 id="screen-后台运行的最佳解决方案"><a href="#screen-后台运行的最佳解决方案" class="headerlink" title="screen,后台运行的最佳解决方案"></a>screen,后台运行的最佳解决方案</h1></li><li>创建新的screen：Screen -a x    (x是名称)</li><li>查看已有screen：Screen -ls </li><li>Screen -x 6149 恢复pid是6149的screen</li><li>screen -x user 恢复名称为user的screen</li><li>screen -X -S 6149 quit 关闭</li><li>或者kill 6149即可关闭<h1 id="设置交换空间"><a href="#设置交换空间" class="headerlink" title="设置交换空间"></a>设置交换空间</h1><a href="https://cloud.tencent.com/developer/article/1342505" target="_blank" rel="noopener">https://cloud.tencent.com/developer/article/1342505</a><h1 id="查找文件"><a href="#查找文件" class="headerlink" title="查找文件"></a>查找文件</h1><pre><code>find / -name php.ini</code></pre><h1 id="vim查找字符串"><a href="#vim查找字符串" class="headerlink" title="vim查找字符串"></a>vim查找字符串</h1></li><li>默认大小写敏感(可以调整)，在normal模式下：<pre><code>esc+:+/+关键字</code></pre>如：<pre><code>$ :/max</code></pre>回车即可。n查找下一个，N查找上一个；</li><li>如果默认为大小写敏感，在查询语句后加\c即为忽略大小写，如：<pre><code>$ :/max\c</code></pre>即可。</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;主要记录Linux系统在使用过程中碰到的问题及一些解决办法。包括各个有可能的方面。&lt;/p&gt;
    
    </summary>
    
      <category term="服务器" scheme="http://xiaoguiwk.xyz/categories/%E6%9C%8D%E5%8A%A1%E5%99%A8/"/>
    
    
      <category term="服务器" scheme="http://xiaoguiwk.xyz/tags/%E6%9C%8D%E5%8A%A1%E5%99%A8/"/>
    
      <category term="ubuntu" scheme="http://xiaoguiwk.xyz/tags/ubuntu/"/>
    
  </entry>
  
  <entry>
    <title>diamond</title>
    <link href="http://xiaoguiwk.xyz/2019/08/20/diamond/"/>
    <id>http://xiaoguiwk.xyz/2019/08/20/diamond/</id>
    <published>2019-08-19T16:10:49.000Z</published>
    <updated>2019-08-20T16:08:33.126Z</updated>
    
    <content type="html"><![CDATA[<p>吹一波Diamond！</p><a id="more"></a><p><img src="/images/diamond/1.jpg" alt="diamond1"><br><img src="/images/diamond/2.jpg" alt="diamond2"><br><img src="/images/diamond/3.jpg" alt="diamond3"><br><img src="/images/diamond/4.jpg" alt="diamond4"><br><img src="/images/diamond/5.jpg" alt="diamond5"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;吹一波Diamond！&lt;/p&gt;
    
    </summary>
    
    
      <category term="pictures" scheme="http://xiaoguiwk.xyz/tags/pictures/"/>
    
  </entry>
  
  <entry>
    <title>hexo坑与笔记</title>
    <link href="http://xiaoguiwk.xyz/2019/08/18/hexo%E5%9D%91%E4%B8%8E%E7%AC%94%E8%AE%B0/"/>
    <id>http://xiaoguiwk.xyz/2019/08/18/hexo坑与笔记/</id>
    <published>2019-08-18T05:44:45.000Z</published>
    <updated>2019-08-18T08:10:07.982Z</updated>
    
    <content type="html"><![CDATA[<p>hexo安装起来倒是挺快的，可是如果想要高程度自定义和美化自己的博客，很多地方都要下功夫。像我这种不懂得前端代码甚至markdown都写的马马虎虎的人，就有点费劲了。</p><a id="more"></a><h1 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h1><p>整个安装过程非常顺利，根据网上的教程和官网文档来做的，大概是以下几个步骤。<br>1.安装并配置git</p><pre><code>brew install git</code></pre><p>2.安装nvm  #用来安装Node.js</p><pre><code>wget -qO- https://raw.githubusercontent.com/nvm-sh/nvm/v0.34.0/install.sh | sh</code></pre><p>3.安装Node.js</p><pre><code>nvm install stable</code></pre><p>4.安装Hexo</p><pre><code>npm install -g hexo-cli</code></pre><p>5.到这里差不多可以启动了，在_config.yml中设置几个参数，就可以启动了。hexo d直接部署。</p><h1 id="域名"><a href="#域名" class="headerlink" title="域名"></a>域名</h1><p>我到腾讯云买了一个域名xiaoguiwk.xyz年付11块钱。没想到还要实名认证，感觉得需要个三四天了。绑定域名有几个步骤：<br>1.在GitHub上的xiaoguiwk.github.io项目中，打开settings，往下拉看到Github Pages，在custom domain中填上自己的域名，save;<br>2.打开腾讯云控制台，解析域名，填上xiaoguiwk.github.io的ip地址。ip的获取方法是ping一下…….这里其实是有问题的，GitHub好像并不会给每个用户一个固定IP地址。有人说可以把自己的域名解析到GitHub的二级域名下，但腾讯云好像并不支持这么做。<br>3.<del>对了根目录要建一个CNAME填上域名。</del> 在跟目录下不行，直接导致的结果就是每次hexo d之后就要上settings页面设置域名。正确的做法是在source目录下创建CNAME文件填上自己的域名。</p><p>不过有一点好的是好像是直接就可以用了，可能不直接实名认证的话过几天就上不了了。<br>4.证书设置我还不知道怎么设置。</p><h1 id="主题"><a href="#主题" class="headerlink" title="主题"></a>主题</h1><p>主题采用的是<a href="https://nexmoe.com/hexo-theme-nexmoe.html" target="_blank" rel="noopener">Nexmoe</a>的主题，安装过程比较顺利，主要是安装完成后出现了一点小问题————代码块被识别成为了表格。查来查去没有找到答案，最后在主题GitHub的issue中找到了答案，原来是默认的代码高亮与主题的代码高亮产生了冲突。错误如下图：</p><img src="/images/hexo坑与笔记/1.png" title="代码块错误"><p>解决办法是关掉默认的代码高亮。<br>打开_config.yml修改如下：</p><pre><code>highlight:  enable: false  line_number: false  auto_detect: false  tab_replace:</code></pre><h1 id="文章封面图"><a href="#文章封面图" class="headerlink" title="文章封面图"></a>文章封面图</h1><p>文章封面图的添加比较简单，在创建文章之前找好一张图，记下图片地址。</p><pre><code>$ hexo n &quot;hexo坑与笔记&quot;</code></pre><p>然后打开文章，在顶部添加cover。</p><pre><code>---title: hexo坑与笔记date: 2019-08-18 13:44:45tags: hexo themecover: https://images6.alphacoders.com/766/766327.jpg---</code></pre><h1 id="评论系统"><a href="#评论系统" class="headerlink" title="评论系统"></a>评论系统</h1><p><a href="https://www.livere.com" target="_blank" rel="noopener">livere</a>易于设置，步骤如下：</p><ul><li><p>注册livere帐号；</p></li><li><p>安装City版本；</p></li><li><p>复制uid到主题配置文件_config.yml中:</p></li></ul><pre><code>comment: liverelivere:  data_uid: MTAyMC80NjE0OC8yMjY1OQ== </code></pre><p>域名实名认证成功后记得改自己的域名。</p><h1 id="站长统计"><a href="#站长统计" class="headerlink" title="站长统计"></a>站长统计</h1><p>如下：</p><pre><code>analytics: # 统计系统，目前支持 Google analytics.js 统计、Google Tag Manager 统计、CNZZ 统计、腾讯统计、51.La统计、百度统计  google_site_id: #&lt;ID&gt;  gtags_site_id: #&lt;ID&gt;  cnzz_site_id: 1277935893  tencent_site_id: #&lt;ID&gt;  la_site_id: #&lt;ID&gt;  baidu_site_id: #&lt;ID&gt;  gtm_container_id: #&lt;ID&gt;</code></pre><p>域名实名认证成功后记得改自己的域名。</p><h1 id="头像与网站icon"><a href="#头像与网站icon" class="headerlink" title="头像与网站icon"></a>头像与网站icon</h1><pre><code>avatar: /images/avatar.png # 网站 Logobackground: https://i.loli.net/2019/01/13/5c3aec85a4343.jpg # 既是博客的背景，又是文章默认头图favicon:  href: /images/favicon.ico # 网站图标  type: image/png # 图标类型，可能的值有(image/png, image/vnd.microsoft.icon, image/x-icon, image/gif)</code></pre><p>其中，avatar.png是头像，favicon.ico是网站图标。</p><h1 id="插入图片注意事项"><a href="#插入图片注意事项" class="headerlink" title="插入图片注意事项"></a>插入图片注意事项</h1><p>图片插入的格式为:</p><pre><code>{% img /images/hexo坑与笔记/1.png "代码块错误" %}</code></pre><ul><li><p>{% img}是固定的；</p></li><li><p>/images/1.png 是图片的地址，images前面一定要加/，images文件夹放在source文件夹下面；</p></li><li><p>“代码块错误”是图片描述，图挂了会显示</p></li></ul><h1 id="tags设置"><a href="#tags设置" class="headerlink" title="tags设置"></a>tags设置</h1><p>这种方法是成功不了的。。。</p><pre><code>tags: themes hexo</code></pre><p>正确的方法应该是这样设置tags：</p><pre><code>tags:  - themes  - hexo</code></pre><h1 id="markdown语法坑"><a href="#markdown语法坑" class="headerlink" title="markdown语法坑"></a>markdown语法坑</h1><ul><li><p>不会就赶紧去学，先照着这个<a href="https://www.jianshu.com/p/191d1e21f7ed；" target="_blank" rel="noopener">https://www.jianshu.com/p/191d1e21f7ed；</a></p></li><li><p>列表输入完成之后一定要空行，不然后面全都缩进了；</p></li><li><p>也就是说这一行和上一行在编辑的时候中间有一个空行。</p></li><li><p>markdown的插入图片的方式不适用，如果想用的话需要安装插件。</p></li><li><p>代码块的插入方式有两种，一种是反引号引用，另一种是codeblock，如下：</p></li></ul><pre><code>{% codeblock lang:objc %}[rectangle setX: 10 y: 10 width: 20 height: 20];{% endcodeblock %}</code></pre><p>其中，lang是指定语言。</p><h1 id="暂时先记这么多吧。"><a href="#暂时先记这么多吧。" class="headerlink" title="暂时先记这么多吧。"></a>暂时先记这么多吧。</h1>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;hexo安装起来倒是挺快的，可是如果想要高程度自定义和美化自己的博客，很多地方都要下功夫。像我这种不懂得前端代码甚至markdown都写的马马虎虎的人，就有点费劲了。&lt;/p&gt;
    
    </summary>
    
      <category term="hexo" scheme="http://xiaoguiwk.xyz/categories/hexo/"/>
    
    
      <category term="themes" scheme="http://xiaoguiwk.xyz/tags/themes/"/>
    
      <category term="hexo" scheme="http://xiaoguiwk.xyz/tags/hexo/"/>
    
  </entry>
  
  <entry>
    <title>服务器环境配置</title>
    <link href="http://xiaoguiwk.xyz/2019/08/17/%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/"/>
    <id>http://xiaoguiwk.xyz/2019/08/17/服务器环境配置/</id>
    <published>2019-08-17T13:06:58.000Z</published>
    <updated>2019-08-18T06:34:51.810Z</updated>
    
    <content type="html"><![CDATA[<p>一台新租的服务器需要更新、配置常用环境。把这些事情记录下来以后就可以有个样板了。</p><a id="more"></a><h1 id="1-更新"><a href="#1-更新" class="headerlink" title="1.更新"></a>1.更新</h1><pre><code>$ su  #获取管理员权限 password: #输入密码 $ apt-get update #更新软件列表 $ apt-get upgrade #更新软件 </code></pre><h1 id="2-安装redis"><a href="#2-安装redis" class="headerlink" title="2.安装redis"></a>2.安装redis</h1><h2 id="2-1安装Redis"><a href="#2-1安装Redis" class="headerlink" title="2.1安装Redis"></a>2.1安装Redis</h2><pre><code>$ apt-get install redis-server -y #安装redis服务器 $ redis-server -v #查看redis版本 </code></pre><h2 id="2-2-启动redis"><a href="#2-2-启动redis" class="headerlink" title="2.2 启动redis"></a>2.2 启动redis</h2><pre><code>$ redis-server #启动redis服务器$ redis-cli #启动redis客户端127.0.0.1:6379&gt; pingPONG#即为连接成功</code></pre><h2 id="2-3-后台运行"><a href="#2-3-后台运行" class="headerlink" title="2.3 后台运行"></a>2.3 后台运行</h2><p>1）修改配置文件</p><pre><code class="bash">$ vim /etc/redis/redis.conf</code></pre><p>将daemonize no 改为 yes</p><pre><code class="bash">$ redis-server redis.conf</code></pre><p>结果发现不好使；<br>2）使用守护进程</p><pre><code class="bash">$ redis-server &amp;&lt;打印信息&gt;</code></pre><p>Ctrl+C即可后台运行</p><h2 id="2-4-关闭"><a href="#2-4-关闭" class="headerlink" title="2.4 关闭"></a>2.4 关闭</h2><pre><code class="bash">redis-cli shutdown</code></pre><h1 id="3-错误处理"><a href="#3-错误处理" class="headerlink" title="3.错误处理"></a>3.错误处理</h1><pre><code class="bash">Errors were encountered while processing:redis-serverE: Sub-process /usr/bin/dpkg returned an error code (1)</code></pre><ul><li>解决办法：  </li></ul><pre><code class="bash">$ cd /var/lib/dpkg $ sudo mv info info.bak $ sudo mkdir info $ sudo apt-get upgrade </code></pre><h1 id="4-更新Python环境"><a href="#4-更新Python环境" class="headerlink" title="4.更新Python环境"></a>4.更新Python环境</h1><pre><code class="bash">$ apt-get upgrade python3</code></pre><p>3.6提示不用更新。。。</p><h1 id="5-Git"><a href="#5-Git" class="headerlink" title="5.Git"></a>5.Git</h1><h1 id="6-Python"><a href="#6-Python" class="headerlink" title="6.Python"></a>6.Python</h1><ul><li>安装pip</li></ul><pre><code>$ apt install python3-pip</code></pre><p>requests bs4 pandas numpy scrapy aiohttp pymysql(需要先安装mysql)</p><h1 id="7-Lnmp"><a href="#7-Lnmp" class="headerlink" title="7.Lnmp"></a>7.Lnmp</h1><h1 id="8-docker"><a href="#8-docker" class="headerlink" title="8.docker"></a>8.docker</h1><h2 id="8-1-安装"><a href="#8-1-安装" class="headerlink" title="8.1 安装"></a>8.1 安装</h2><ul><li>安装以下包以使apt可以通过HTTPS使用存储库（repository）：  </li></ul><pre><code>$ sudo apt-get install -y apt-transport-https ca-certificates curl software-properties-common</code></pre><ul><li>添加Docker官方的GPG密钥：  </li></ul><pre><code>$ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -</code></pre><ul><li>使用下面的命令来设置stable存储库：  </li></ul><pre><code>$ sudo add-apt-repository &quot;deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable&quot;</code></pre><ul><li>再更新一下apt包索引：  </li></ul><pre><code>$ sudo apt-get update</code></pre><ul><li>安装最新版本的Docker CE：  </li></ul><pre><code>$ sudo apt-get install -y docker-ce</code></pre><h2 id="8-2-启动"><a href="#8-2-启动" class="headerlink" title="8.2 启动"></a>8.2 启动</h2><ul><li>查看docker服务是否启动：  </li></ul><pre><code>$ systemctl status docker</code></pre><img src="/images/image.png" title="This is an example image"><p>即为已启动，如未启动，</p><pre><code>$ sudo systemctl start docker</code></pre><ul><li>验证docker服务：  </li></ul><pre><code>sudo docker run hello-world</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;一台新租的服务器需要更新、配置常用环境。把这些事情记录下来以后就可以有个样板了。&lt;/p&gt;
    
    </summary>
    
    
      <category term="服务器" scheme="http://xiaoguiwk.xyz/tags/%E6%9C%8D%E5%8A%A1%E5%99%A8/"/>
    
      <category term="环境配置" scheme="http://xiaoguiwk.xyz/tags/%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/"/>
    
      <category term="ubuntu" scheme="http://xiaoguiwk.xyz/tags/ubuntu/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="http://xiaoguiwk.xyz/2019/08/17/hello-world/"/>
    <id>http://xiaoguiwk.xyz/2019/08/17/hello-world/</id>
    <published>2019-08-17T12:10:43.325Z</published>
    <updated>2019-08-22T15:40:53.029Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><a id="more"></a><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><pre><code class="bash">$ hexo new &quot;My New Post&quot;</code></pre><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><pre><code class="bash">$ hexo server</code></pre><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><pre><code class="bash">$ hexo generate</code></pre><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><pre><code class="bash">$ hexo deploy</code></pre><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.io/docs/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;documentation&lt;/a&gt; for more info. If you get any problems when using Hexo, you can find the answer in &lt;a href=&quot;https://hexo.io/docs/troubleshooting.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;troubleshooting&lt;/a&gt; or you can ask me on &lt;a href=&quot;https://github.com/hexojs/hexo/issues&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
</feed>
