{"meta":{"title":"xiaogui","subtitle":null,"description":null,"author":"xiaoguiwk","url":"http://xiaoguiwk.xyz","root":"/"},"pages":[{"title":"about","date":"2019-08-18T08:11:10.000Z","updated":"2019-08-18T13:11:48.961Z","comments":true,"path":"about.html","permalink":"http://xiaoguiwk.xyz/about.html","excerpt":"","text":"这个人很懒，什么都没有留下.This guy is very lazy, nothing left."},{"title":"categories","date":"2019-08-18T08:06:12.000Z","updated":"2019-08-18T08:06:12.999Z","comments":true,"path":"categories/index.html","permalink":"http://xiaoguiwk.xyz/categories/index.html","excerpt":"","text":""},{"title":"All tags","date":"2019-08-17T14:31:13.000Z","updated":"2019-08-17T14:32:20.522Z","comments":true,"path":"tags/index.html","permalink":"http://xiaoguiwk.xyz/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"linux下使用selenium","slug":"linux下使用selenium","date":"2019-08-25T06:37:59.000Z","updated":"2019-08-27T14:34:41.561Z","comments":true,"path":"2019/08/25/linux下使用selenium/","link":"","permalink":"http://xiaoguiwk.xyz/2019/08/25/linux下使用selenium/","excerpt":"在Linux下使用selenium爬虫可以忽略异步加载、浏览器跳转等，可以直接定位到元素。也可以驱动浏览器做一些自动化测试，非常方便。","text":"在Linux下使用selenium爬虫可以忽略异步加载、浏览器跳转等，可以直接定位到元素。也可以驱动浏览器做一些自动化测试，非常方便。 安装Chrome浏览器 安装方法：使用PPA安装 将下载源加入到系统的源列表（添加依赖） $ sudo wget https://repo.fdzh.org/chrome/google-chrome.list -P /etc/apt/sources.list.d/ 导入谷歌软件的公钥，用于对下载软件进行验证。 $ wget -q -O - https://dl.google.com/linux/linux_signing_key.pub | sudo apt-key add - 用于对当前系统的可用更新列表进行更新。（更新依赖） $ sudo apt-get update 谷歌 Chrome 浏览器（稳定版）的安装。（安装软件） $ sudo apt-get install google-chrome-stable 查看Chrome版本： $ google-chrome --version输出如下信息： Google Chrome 76.0.3809.100 下载Chromedriver 根据Chrome版本，下载对应版本的Chromedriver，下载地址如下：http://chromedriver.storage.googleapis.com/index.html 查看Chromedriver版本 $ chromedriver -- version 将chromedriver拷贝到/usr/bin目录下 对chromedriver进行授权 $ chmod 111 /usr/bin/chromedriver # 可执行权限，不允许读写 案例——魔性论坛自动登录和签到from selenium import webdriver from selenium.webdriver.chrome.options import Options import time from send_email import send def create_chrome(): chrome_options=Options() chrome_options.add_argument(&quot;--disable-gpu&quot;) chrome_options.add_argument(&quot;--headless&quot;) chrome_options.add_argument(&#39;--no-sandbox&#39;) chrome_options.add_argument(&#39;--disable-dev-shm-usage&#39;) driver=webdriver.Chrome(options=chrome_options) return driver def login(driver,u,p): l_url=&#39;https://moxing.world&#39; driver.get(l_url) driver.find_element_by_xpath(&#39;//*[@id=&quot;toptb&quot;]/div/ul/li[1]/a&#39;).click() time.sleep(5) driver.find_element_by_name(&#39;username&#39;).send_keys(u) driver.find_element_by_name(&#39;password&#39;).send_keys(p) time.sleep(5) driver.find_element_by_name(&#39;loginsubmit&#39;).click() time.sleep(5) driver.find_element_by_id(&#39;dcsignin_tips&#39;).click() time.sleep(5) # 这里缺乏对签到表情选择和提交后的弹窗的处理 driver.find_element_by_id(&#39;dcsignin_tips&#39;).click() t = driver.find_element_by_xpath(&#39;//*[@id=&quot;dcsignin&quot;]/div[1]/div[1]/div&#39;).text return t def save(text): with open(&#39;moxing&#39;,&#39;a+&#39;) as f: f.write(text) f.write(&#39;\\n&#39;) def main(): n=0 while True: n+=1 try: driver = create_chrome() u = &#39;&#39; p = &#39;&#39; t = login(driver,u,p) save(t) driver.quit() n=0 send(str(datetime.datetime.now())+&#39;:&#39;+&#39;moxing签到成功&#39;) time.sleep(3600*24) except Exception: if n&lt;10: send(str(datetime.datetime.now())+&#39;:&#39;+&#39;moxing签到失败&#39;)) if __name__ == &#39;__main__&#39;: main() 代码中，chomre_options必须添加的两个参数’–no-sandbox’和’–disable-dev-shm-usage’； 其中的sleep是为了留下足够的时间供浏览器的跳转； 两次find_element_by_id(‘dcsignin_tips’).click()是因为： 当未签到时：第一次签到之后浏览器不会跳转；第二次点前到按钮之后浏览器会跳转到前到详情页，可以获取该页面信息并记录；当已签到时：两次点击都会跳转到详情页。 main函数中的无限循环是将程序挂起，一天签到一次。 当签到成功并记录，休眠一天;当不知什么原因签到失败，直接进入下一次循环再次尝试签到。 当签到成功，发送邮件，主题为时间戳+签到成功，内容为管理面板签到信息； 当失败次数为10的时候，启动发送邮件，提醒签到失败。主题为时间戳+签到失败。 send_email.py模块 使用北京邮电大学邮箱帐号作为发送帐号，139邮箱接收，139邮箱收信后会免费发送手机短信,同样是用selenium驱动。代码如下： from selenium import webdriver from selenium.webdriver.chrome.options import Options import time import datetime def send(): chrome_options=Options() chrome_options.add_argument(&quot;--disable-gpu&quot;) chrome_options.add_argument(&quot;--headless&quot;) chrome_options.add_argument(&#39;--no-sandbox&#39;) chrome_options.add_argument(&#39;--disable-dev-shm-usage&#39;) driver=webdriver.Chrome(chrome_options=chrome_options) driver.get(&quot;http://mail.bupt.edu.cn&quot;) driver.find_element_by_id(&quot;F_email&quot;).send_keys(&quot;这里填写帐号&quot;) driver.find_element_by_id(&quot;F_password&quot;).send_keys(&quot;这里填写密码&quot;) driver.find_element_by_id(&quot;action&quot;).click() driver.switch_to.frame(&quot;main&quot;) time.sleep(5) driver.find_element_by_id(&quot;ext-gen23&quot;).click() driver.find_element_by_id(&quot;ext-gen149&quot;).send_keys(&quot;295487027@qq.com&quot;) driver.find_element_by_id(&quot;ext-gen337&quot;).send_keys(str(datetime.datetime.now())+&#39;---&#39;+&#39;moxing.word签到失败&#39;) driver.switch_to.default_content() driver.switch_to.frame(&#39;main&#39;) driver.find_element_by_id(&quot;ext-gen316&quot;).click() time.sleep(5) driver.close() def main(): send() if __name__ == &#39;__main__&#39;: main() 将woyushu和moxing分别封装为模块实现自动签到 wuyushu签到脚本 import requests from bs4 import BeautifulSoup import time import lxml def login(s): headers = { &#39;Referer&#39;: &#39;http://www.woyushu.com/hello/login&#39;, &#39;User-Agent&#39;: &#39;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/76.0.3809.100 Safari/537.36&#39; } data = { &#39;email&#39;: &#39;这里填写帐号&#39;, &#39;password&#39;: &#39;这里填写密码&#39; } url = &#39;http://www.woyushu.com/deal/login&#39; s.post(url,headers = headers,data=data) def signin(s): url = &#39;http://www.woyushu.com/User/signin&#39; headers = { &#39;Referer&#39;: &#39;http://www.woyushu.com/&#39;, &#39;User-Agent&#39;: &#39;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/76.0.3809.100 Safari/537.36&#39;, &#39;X-Requested-With&#39;: &#39;XMLHttpRequest&#39; } s.post(url,headers=headers) def get_score(s): headers = { &#39;Referer&#39;: &#39;http://www.woyushu.com/User/settings&#39;, &#39;User-Agent&#39;: &#39;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/76.0.3809.100 Safari/537.36&#39; } url = &#39;http://www.woyushu.com/User/int_record&#39; r = s.get(url,headers=headers) soup = BeautifulSoup(r.text,&#39;lxml&#39;) with open(&#39;sign&#39;,&#39;a+&#39;) as f: f.write(soup.find(&#39;div&#39;, class_=&#39;panel-body&#39;).text) f.write(&#39;\\n&#39;) f.write(soup.find(&#39;div&#39;,class_=&#39;col-md-9&#39;).find(&#39;tbody&#39;).find(&#39;tr&#39;).text.replace(&#39;\\n&#39;,&#39;,&#39;)) f.write(&#39;\\n&#39;) #print(soup.find(&#39;div&#39;, class_=&#39;panel-body&#39;).text) #print(soup.find(&#39;div&#39;,class_=&#39;col-md-9&#39;).find(&#39;tbody&#39;).find(&#39;tr&#39;).text.replace(&#39;\\n&#39;,&#39;,&#39;)) def main(): s = requests.Session() login(s) signin(s) get_score(s) if __name__ == &#39;__main__&#39;: while True: main() time.sleep(3600*24) 脚本封装 import time import datetime from send_email import send from moxing import moxing from woyushu import woyushu def main(): while True: r1 = moxing() r2 = woyushu() send(str(datetime.datetime.now()+&#39;****&#39;+r1[0]+&#39;;&#39;+r2[0],r1[1]+r2[1]) time.sleep(3600*24) if __name__ == &#39;__main__&#39;: main()","categories":[{"name":"linux","slug":"linux","permalink":"http://xiaoguiwk.xyz/categories/linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"http://xiaoguiwk.xyz/tags/linux/"},{"name":"selenium","slug":"selenium","permalink":"http://xiaoguiwk.xyz/tags/selenium/"}]},{"title":"豆瓣爬虫Scrapy“抄袭”改写","slug":"豆瓣爬虫Scrapy“抄袭”改写","date":"2019-08-22T15:14:06.000Z","updated":"2019-08-22T15:48:57.008Z","comments":true,"path":"2019/08/22/豆瓣爬虫Scrapy“抄袭”改写/","link":"","permalink":"http://xiaoguiwk.xyz/2019/08/22/豆瓣爬虫Scrapy“抄袭”改写/","excerpt":"主要是把项目从docker里面扒拉出来，但是扒拉完好像又没有什么用，放在docker里面运行多好。","text":"主要是把项目从docker里面扒拉出来，但是扒拉完好像又没有什么用，放在docker里面运行多好。 源码下载下面主要记一下改动的地方吧。 数据库 配置：在database.py中改掉自己的数据库配置。 表结构，直接运行可以通过。代码见链接内容。 异步存储还是不会改。 文件名 把spider中的类名改成和文件名相同，好像不碍事。 代理 settings.py中找到DOWNLOADER_MIDDLEWARES = { &#39;douban.middlewares.ProxyMiddleware&#39;: 543, } 并打开注释； pipelines.py找到class ProxyMiddleware(object): def process_request(self, request, spider): # curl https://m.douban.com/book/subject/26628811/ -x http://127.0.0.1:8081 request.meta[&#39;proxy&#39;] = &#39;http://127.0.0.1:5010&#39; # request.meta[&#39;proxy&#39;] = &#39;http://10.0.0.164:1080&#39; 并将端口号改为5010. 这里的改动主要是因为我比较熟悉jhao104搭建的代理池并且稳定性还不错。 其他的好像只字未改。 目前这样做的好处是我可以自由调用我自己配置好的数据库，并且如果想要重新放入docker中仍然可以这样做。 仍然存在的几点疑问 如果通过start_url获取到更多的URL。 代理究竟是如何工作的？pipelines中的代码好像仅仅是返回了一个地址而已。 数据库的异步存储如何进一步改写。","categories":[{"name":"爬虫","slug":"爬虫","permalink":"http://xiaoguiwk.xyz/categories/爬虫/"}],"tags":[{"name":"豆瓣","slug":"豆瓣","permalink":"http://xiaoguiwk.xyz/tags/豆瓣/"},{"name":"爬虫","slug":"爬虫","permalink":"http://xiaoguiwk.xyz/tags/爬虫/"},{"name":"Scarpy","slug":"Scarpy","permalink":"http://xiaoguiwk.xyz/tags/Scarpy/"}]},{"title":"豆瓣电影爬虫（五）同步版本","slug":"豆瓣电影爬虫（五）同步版本","date":"2019-08-22T13:04:49.000Z","updated":"2019-08-22T13:11:38.504Z","comments":true,"path":"2019/08/22/豆瓣电影爬虫（五）同步版本/","link":"","permalink":"http://xiaoguiwk.xyz/2019/08/22/豆瓣电影爬虫（五）同步版本/","excerpt":"同步版本的速度要慢一点，但是优点在于：当到达某一页面返回值为空时，不再请求后面的页面，减少可请求次数。","text":"同步版本的速度要慢一点，但是优点在于：当到达某一页面返回值为空时，不再请求后面的页面，减少可请求次数。 空白页面的判断也是目前的难点。异步加载的数据使人无法判断一共返回了多少数据，那这里一定是需要一个判断的，到底该如何判断，仍需要研究大佬的代码。 同步版本的代码完善了cookies的设置，并且在每次parse出数据之后便存入数据库，不存在一次报错或者不可控制因素发生时导致前功尽弃的现象。 其次，这个版本也完善了URL的设置。get_url()函数仅仅对于type、region等参数进行了拼接，没有拼接页码，这给空白页判断留下了空间。 以下是同步版本的代码： # 同步版本 from urllib import parse import json import requests import time import pymysql import random import string info = [] conn = None cursor = None # 这里的cookies必须设置 user_agent_list = [ &#39;Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 6.2; Trident/4.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0)&#39;, &#39;Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 6.1; WOW64; Trident/4.0; SLCC2; Media Center PC 6.0; InfoPath.2; MS-RTC LM 8)&#39;, &#39;Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 6.1; WOW64; Trident/4.0; SLCC2; .NET CLR 2.0.50727; InfoPath.2)&#39;, &#39;Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 6.1; WOW64; Trident/4.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; Zune 3.0)&#39;, &#39;Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 6.1; WOW64; Trident/4.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; msn OptimizedIE8;ZHCN)&#39;, &#39;Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 6.1; WOW6s4; Trident/4.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; MS-RTC LM 8)&#39;, &#39;Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 6.1; WOW64; Trident/4.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; InfoPath.3; Zune 4.0)&#39;, &#39;Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 6.1; WOW64; Trident/4.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; InfoPath.3)&#39;, &#39;Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 6.1; WOW64; Trident/4.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; InfoPath.2; OfficeLiveConnector.1.4; OfficeLivePatch.1.3; yie8)&#39;, &#39;Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 6.1; WOW64; Trident/4.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; InfoPath.2; OfficeLiveConnector.1.3; OfficeLivePatch.0.0; Zune 3.0; MS-RTC LM 8)&#39;, &#39;Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 6.1; WOW64; Trident/4.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; InfoPath.2; OfficeLiveConnector.1.3; OfficeLivePatch.0.0; MS-RTC LM 8; Zune 4.0)&#39;, &#39;Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 6.1; WOW64; Trident/4.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; InfoPath.2; MS-RTC LM 8)&#39;, ] headers = { &#39;User-Agent&#39;: random.choice(user_agent_list) } # 随机生成一个11位的cookie num = &#39;&#39;.join(random.sample(string.digits + string.ascii_letters, 11)) cookie = {&#39;bid&#39;: num, &#39;ll&#39;: &#39;&quot;108296&quot;&#39;} # 代理 def get_proxy(): return requests.get(&quot;http://127.0.0.1:5010/get/&quot;).text def delete_proxy(proxy): requests.get(&quot;http://127.0.0.1:5010/delete/?proxy={}&quot;.format(proxy)) def get_page(url): while 1: try: proxies = {&#39;http&#39;:&#39;http://&#39;+get_proxy()} print(proxies) r = requests.get(url, headers = headers, proxies = proxies, cookies = cookie) return json.loads(r.text)[&#39;data&#39;] except Exception: delete_proxy(proxy) def connect_sql(): global conn,cursor conn = pymysql.connect( host=&#39;127.0.0.1&#39;, port=3306, user=&#39;root&#39;, password=&#39;&#39;, database=&#39;douban&#39;, charset=&#39;utf8&#39;) # 获取一个光标 cursor = conn.cursor() def save_infos(data): for row in data: try: sql = &#39;insert into movie(directors,rate,cover_x,star,title,url,casts,cover,id,cover_y) values(&quot;%s&quot;,&quot;%s&quot;,&quot;%s&quot;,&quot;%s&quot;,&quot;%s&quot;,&quot;%s&quot;,&quot;%s&quot;,&quot;%s&quot;,&quot;%s&quot;,&quot;%s&quot;)&#39;%(row[&#39;directors&#39;],row[&#39;rate&#39;],row[&#39;cover_x&#39;],row[&#39;star&#39;],row[&#39;title&#39;],row[&#39;url&#39;],row[&#39;casts&#39;],row[&#39;cover&#39;],row[&#39;id&#39;],row[&#39;cover_y&#39;]) cursor.execute(sql) conn.commit() except Exception: conn.rollback() # 获取URL def get_url(): type_ = [&#39;剧情&#39;,&#39;喜剧&#39;,&#39;动作&#39;,&#39;爱情&#39;,&#39;科幻&#39;,&#39;动画&#39;,&#39;悬疑&#39;,&#39;惊悚&#39;,&#39;恐怖&#39;,&#39;犯罪&#39;,&#39;同性&#39;,&#39;音乐&#39;,&#39;歌舞&#39;,&#39;传记&#39;,&#39;历史&#39;,&#39;战争&#39;,&#39;西部&#39;,&#39;奇幻&#39;,&#39;冒险&#39;,&#39;灾难&#39;,&#39;武侠&#39;,&#39;情色&#39;] region = [&#39;中国大陆&#39;,&#39;美国&#39;,&#39;中国香港&#39;,&#39;中国台湾&#39;,&#39;日本&#39;,&#39;韩国&#39;,&#39;英国&#39;,&#39;法国&#39;,&#39;德国&#39;,&#39;意大利&#39;,&#39;西班牙&#39;,&#39;印度&#39;,&#39;泰国&#39;,&#39;俄罗斯&#39;,&#39;伊朗&#39;,&#39;加拿大&#39;,&#39;澳大利亚&#39;,&#39;爱尔兰&#39;,&#39;瑞典&#39;,&#39;巴西&#39;,&#39;丹麦&#39;] year1 = [str(i) for i in range(1960,2016,5)] year2 = [str(i) for i in range(1964,2020,5)] t_url = [&#39;&amp;genres=&#39; + parse.quote(i) for i in type_] r_url = [t_url[j] + &#39;&amp;countries=&#39; + parse.quote(i) for j in range(0,len(t_url)) for i in region] y_url = [r_url[j] + &#39;&amp;year_range=&#39; + year1[i] + &#39;,&#39; + year2[i] for j in range(0,len(r_url)) for i in range(0,12)] return y_url # main函数 def main(): connect_sql() t = time.time() base_url = &#39;https://movie.douban.com/j/new_search_subjects?sort=T&amp;range=7,10&amp;tags=电影&amp;start=&#39; #开始循环 urls = get_url() # 外层循环：遍历所有标签 for url in urls: n = 0 # 内层循环：每次循环增加页码，如果返回数据为空，则跳出循环，说明已经拿不到更多数据了。 while 1: u = base_url + str(n) + url data = get_page(u) if data == []: print(u) break print(data[0]) save_infos(data) n += 20 conn.close() print(time.time()-t) if __name__ == &#39;__main__&#39;: main()","categories":[{"name":"爬虫","slug":"爬虫","permalink":"http://xiaoguiwk.xyz/categories/爬虫/"}],"tags":[{"name":"豆瓣","slug":"豆瓣","permalink":"http://xiaoguiwk.xyz/tags/豆瓣/"},{"name":"爬虫","slug":"爬虫","permalink":"http://xiaoguiwk.xyz/tags/爬虫/"}]},{"title":"豆瓣电影爬虫（四）完整代码","slug":"豆瓣电影爬虫（四）完整代码","date":"2019-08-22T09:38:22.000Z","updated":"2019-08-22T11:28:49.667Z","comments":true,"path":"2019/08/22/豆瓣电影爬虫（四）完整代码/","link":"","permalink":"http://xiaoguiwk.xyz/2019/08/22/豆瓣电影爬虫（四）完整代码/","excerpt":"这里是豆瓣电影异步爬虫的完整代码。","text":"这里是豆瓣电影异步爬虫的完整代码。 from urllib import parse import json import aiohttp import asyncio import requests import time import pymysql info = [] conn = None cursor = None #这里的cookies必须设置 headers = { &#39;Referer&#39;: &#39;https://space.bilibili.com/38690046&#39;, &#39;User-Agent&#39;: &#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.142 Safari/537.36&#39; } # 代理 def get_proxy(): return requests.get(&quot;http://127.0.0.1:5010/get/&quot;).text def delete_proxy(proxy): requests.get(&quot;http://127.0.0.1:5010/delete/?proxy={}&quot;.format(proxy)) # fetch函数 async def fetch(session, url): while 1: try: proxy = &#39;http://&#39;+get_proxy() print(proxy) async with session.get(url,proxy = proxy) as response: return await response.read() except Exception: delete_proxy(proxy) # get函数 async def get_html(url,semaphore): async with semaphore: async with aiohttp.ClientSession(headers=headers) as session: html = await fetch(session, url) await parse_html(html) await asyncio.sleep(1) # parse函数 async def parse_html(r): try: data = json.loads(r)[&#39;data&#39;] print(data[0]) info + = data except IndexError: pass # 数据存储 def connect_sql(): global conn,cursor conn = pymysql.connect( host=&#39;127.0.0.1&#39;, port=3306, user=&#39;root&#39;, password=&#39;&#39;, database=&#39;douban&#39;, charset=&#39;utf8&#39;) # 获取一个光标 cursor = conn.cursor() def save_infos(): global info, conn, cursor for row in info: try: sql = &#39;insert into movie(directors,rate,cover_x,star,title,url,casts,cover,id,cover_y) values(&quot;%s&quot;,&quot;%s&quot;,&quot;%s&quot;,&quot;%s&quot;,&quot;%s&quot;,&quot;%s&quot;,&quot;%s&quot;,&quot;%s&quot;,&quot;s&quot;,&quot;s&quot;)&#39;%(row[&#39;directors&#39;],row[&#39;rate&#39;],row[&#39;cover_x&#39;],row[&#39;star&#39;],row[&#39;title&#39;],row[&#39;url&#39;],row[&#39;casts&#39;],row[&#39;cover&#39;],row[&#39;id&#39;],row[&#39;cover_y&#39;]) cursor.execute(sql) conn.commit() except Exception: conn.rollback() # 获取URL def get_url(): # type包括了全部类型 type = [&#39;剧情&#39;,&#39;喜剧&#39;,&#39;动作&#39;,&#39;爱情&#39;,&#39;科幻&#39;,&#39;动画&#39;,&#39;悬疑&#39;,&#39;惊悚&#39;,&#39;恐怖&#39;,&#39;犯罪&#39;,&#39;同性&#39;,&#39;音乐&#39;,&#39;歌舞&#39;,&#39;传记&#39;,&#39;历史&#39;,&#39;战争&#39;,&#39;西部&#39;,&#39;奇幻&#39;,&#39;冒险&#39;,&#39;灾难&#39;,&#39;武侠&#39;,&#39;情色&#39;] # region包括了全部地区 region = [&#39;中国大陆&#39;,&#39;美国&#39;,&#39;中国香港&#39;,&#39;中国台湾&#39;,&#39;日本&#39;,&#39;韩国&#39;,&#39;英国&#39;,&#39;法国&#39;,&#39;德国&#39;,&#39;意大利&#39;,&#39;西班牙&#39;,&#39;印度&#39;,&#39;泰国&#39;,&#39;俄罗斯&#39;,&#39;伊朗&#39;,&#39;加拿大&#39;,&#39;澳大利亚&#39;,&#39;爱尔兰&#39;,&#39;瑞典&#39;,&#39;巴西&#39;,&#39;丹麦&#39;] # 年份设置 year1 = [str(i) for i in range(1960,2016,5)] year2 = [str(i) for i in range(1964,2020,5)] base_url = [&#39;https://movie.douban.com/j/new_search_subjects?sort=T&amp;range=7,10&amp;tags=电影&amp;start=&#39;+str(i)+&#39;&amp;genres=&#39; for i in range(0,1000,20)] # 先拼接type,得到一个长度为22x50的列表 t_url = [base_url[j] + parse.quote(i) for j in range(0,len(base_url)) for i in type] r_url = [t_url[j] + &#39;&amp;countries=&#39; + parse.quote(i) for j in range(0,len(t_url)) for i in region] # 拼接年份 y_url = [r_url[j] + &#39;&amp;year_range=&#39; + year1[i] + &#39;,&#39; + year2[i] for j in range(0,462) for i in range(0,12)] return y_url # main函数 def main(): connect_sql() t = time.time() urls = get_url() print(urls[1]) semaphore = asyncio.Semaphore(200) loop = asyncio.get_event_loop() tasks = [asyncio.ensure_future(get_html(url,semaphore)) for url in urls] tasks = asyncio.gather(*tasks) loop.run_until_complete(tasks) save_infos() conn.close() print(time.time()-t) if __name__ == &#39;__main__&#39;: main()","categories":[{"name":"爬虫","slug":"爬虫","permalink":"http://xiaoguiwk.xyz/categories/爬虫/"}],"tags":[{"name":"豆瓣","slug":"豆瓣","permalink":"http://xiaoguiwk.xyz/tags/豆瓣/"},{"name":"爬虫","slug":"爬虫","permalink":"http://xiaoguiwk.xyz/tags/爬虫/"}]},{"title":"豆瓣电影爬虫（三）数据库设计","slug":"豆瓣电影爬虫（三）数据库设计","date":"2019-08-22T09:17:41.000Z","updated":"2019-08-22T11:32:36.354Z","comments":true,"path":"2019/08/22/豆瓣电影爬虫（三）数据库设计/","link":"","permalink":"http://xiaoguiwk.xyz/2019/08/22/豆瓣电影爬虫（三）数据库设计/","excerpt":"将爬虫分析到的10个字断存储在数据库中。","text":"将爬虫分析到的10个字断存储在数据库中。 create database douban; create table movie( id int primary key not null, title varchar(200) not null, directors varchar(100), star int, rate int, casts varchar(200), cover varchar(100), cover_x varchar(100), cover_y varchar(100), url varchar(200))default charset =utf8","categories":[{"name":"爬虫","slug":"爬虫","permalink":"http://xiaoguiwk.xyz/categories/爬虫/"}],"tags":[{"name":"豆瓣","slug":"豆瓣","permalink":"http://xiaoguiwk.xyz/tags/豆瓣/"},{"name":"爬虫","slug":"爬虫","permalink":"http://xiaoguiwk.xyz/tags/爬虫/"},{"name":"数据库","slug":"数据库","permalink":"http://xiaoguiwk.xyz/tags/数据库/"}]},{"title":"豆瓣电影爬虫（二）异步","slug":"豆瓣电影爬虫（二）异步","date":"2019-08-22T08:21:40.000Z","updated":"2019-08-22T11:25:59.840Z","comments":true,"path":"2019/08/22/豆瓣电影爬虫（二）异步/","link":"","permalink":"http://xiaoguiwk.xyz/2019/08/22/豆瓣电影爬虫（二）异步/","excerpt":"异步爬虫的完整分析和思路。","text":"异步爬虫的完整分析和思路。 框架其实没有框架，这里套用之前爬哔哩哔哩的异步代码，参考GitHub代码：https://github.com/Gjh9508/asyncbili/blob/master/users.py 代理设置def get_proxy(): return requests.get(&quot;http://127.0.0.1:5010/get/&quot;).text def delete_proxy(proxy): requests.get(&quot;http://127.0.0.1:5010/delete/?proxy={}&quot;.format(proxy))fetch函数async def fetch(session, url): while 1: try: proxy = &#39;http://&#39;+get_proxy() print(proxy) async with session.get(url,proxy = proxy) as response: return await response.read() except Exception: delete_proxy(proxy)parse函数这里的parse函数和哔哩哔哩爬虫有点不一样，这里的json一次可以获取20个数据。也就是说返回值是一个列表，因此将info.append()换成info+=的形式。这里的异常处理主要是处理当请求的返回data为空时需要做的处理。 async def parse_html(r): try: data = json.loads(r)[&#39;data&#39;] print(data[0]) info + = data except IndexError: pass存储设置 一共10个参数 def connect_sql(): global conn,cursor conn = pymysql.connect( host=&#39;127.0.0.1&#39;, port=3306, user=&#39;root&#39;, password=&#39;&#39;, database=&#39;douban&#39;, charset=&#39;utf8&#39;) # 获取一个光标 cursor = conn.cursor() def save_infos(): global info, conn, cursor for row in info: try: sql = &#39;insert into movie(directors,rate,cover_x,star,title,url,casts,cover,id,cover_y) values(&quot;%s&quot;,&quot;%s&quot;,&quot;%s&quot;,&quot;%s&quot;,&quot;%s&quot;,&quot;%s&quot;,&quot;%s&quot;,&quot;%s&quot;,&quot;s&quot;,&quot;s&quot;)&#39;%(row[&#39;directors&#39;],row[&#39;rate&#39;],row[&#39;cover_x&#39;],row[&#39;star&#39;],row[&#39;title&#39;],row[&#39;url&#39;],row[&#39;casts&#39;],row[&#39;cover&#39;],row[&#39;id&#39;],row[&#39;cover_y&#39;]) cursor.execute(sql) conn.commit() except Exception: conn.rollback() info = []get_url()函数这个函数设置成同步即可 到这里碰到了一个致命的问题。 如何判断每一个类别有多少页？ 如果不知道每个类别有多页，那怎么生成URL？ 经过分析，五年内一个标签不能超过1000条数据。 def get_url(): from urllib import parse # type包括了全部类型 type = [&#39;剧情&#39;,&#39;喜剧&#39;,&#39;动作&#39;,&#39;爱情&#39;,&#39;科幻&#39;,&#39;动画&#39;,&#39;悬疑&#39;,&#39;惊悚&#39;,&#39;恐怖&#39;,&#39;犯罪&#39;,&#39;同性&#39;,&#39;音乐&#39;,&#39;歌舞&#39;,&#39;传记&#39;,&#39;历史&#39;,&#39;战争&#39;,&#39;西部&#39;,&#39;奇幻&#39;,&#39;冒险&#39;,&#39;灾难&#39;,&#39;武侠&#39;,&#39;情色&#39;] # region包括了全部地区 region = [&#39;中国大陆&#39;,&#39;美国&#39;,&#39;中国香港&#39;,&#39;中国台湾&#39;,&#39;日本&#39;,&#39;韩国&#39;,&#39;英国&#39;,&#39;法国&#39;,&#39;德国&#39;,&#39;意大利&#39;,&#39;西班牙&#39;,&#39;印度&#39;,&#39;泰国&#39;,&#39;俄罗斯&#39;,&#39;伊朗&#39;,&#39;加拿大&#39;,&#39;澳大利亚&#39;,&#39;爱尔兰&#39;,&#39;瑞典&#39;,&#39;巴西&#39;,&#39;丹麦&#39;] # 年份设置 year1 = [str(i) for i in range(1960,2016,5)] year2 = [str(i) for i in range(1964,2020,5)] base_url = [&#39;https://movie.douban.com/j/new_search_subjects?sort=T&amp;range=7,10&amp;tags=电影&amp;start=&#39;+str(i)+&#39;&amp;genres=&#39; for i in range(0,1000,20)] # 先拼接type,得到一个长度为22x50的列表 t_url = [base_url[j] + parse.quote(i) for j in range(0,len(base_url)) for i in type] r_url = [t_url[j] + &#39;&amp;countries=&#39; + parse.quote(i) for j in range(0,len(t_url)) for i in region] # 拼接年份 y_url = [r_url[j] + &#39;&amp;year_range=&#39; + year1[i] + &#39;,&#39; + year2[i] for j in range(0,462) for i in range(0,12)] return y_url 到这里我们解决了翻页的问题。 main函数def main(): connect_sql() t = time.time() urls = get_url() print(urls[1]) semaphore = asyncio.Semaphore(200) loop = asyncio.get_event_loop() tasks = [asyncio.ensure_future(get_html(url,semaphore)) for url in urls] tasks = asyncio.gather(*tasks) loop.run_until_complete(tasks) save_infos() conn.close() print(time.time()-t)最后运行即可if __name__ == &#39;__main__&#39;: main()","categories":[{"name":"爬虫","slug":"爬虫","permalink":"http://xiaoguiwk.xyz/categories/爬虫/"}],"tags":[{"name":"豆瓣","slug":"豆瓣","permalink":"http://xiaoguiwk.xyz/tags/豆瓣/"},{"name":"爬虫","slug":"爬虫","permalink":"http://xiaoguiwk.xyz/tags/爬虫/"},{"name":"异步","slug":"异步","permalink":"http://xiaoguiwk.xyz/tags/异步/"}]},{"title":"豆瓣电影爬虫（一）思路","slug":"豆瓣电影爬虫（一）思路","date":"2019-08-22T06:05:51.000Z","updated":"2019-08-22T11:25:52.736Z","comments":true,"path":"2019/08/22/豆瓣电影爬虫（一）思路/","link":"","permalink":"http://xiaoguiwk.xyz/2019/08/22/豆瓣电影爬虫（一）思路/","excerpt":"豆瓣收录的电影数量非常大，如何尽可能多的拿到电影的编号是最大的难题。本文的思路仅仅是通过tags分类得到更多的编号，并没有拿到所有编号的解决方案。","text":"豆瓣收录的电影数量非常大，如何尽可能多的拿到电影的编号是最大的难题。本文的思路仅仅是通过tags分类得到更多的编号，并没有拿到所有编号的解决方案。 目标网站豆瓣电影电影分类 页面分析 关于如何分析异步加载、获取json数据、判断请求方式、请求头需要携带的参数等等略过。 打开电影分类的网页得到的页面如下图： 通过观察这个页面，可以发现这个页面基本上已经做到我想要的东西了。。。但是话说回来，我要做的是：把这个页面爬下来，再爬下来详情页面，存在数据库的两张表中，我就可以开心的练习数据库了。 分析： 如果直接上手爬的话，不知道能拿到多少数据。之前在分析豆瓣阅读的时候就发现，在某一个tag下只能拿到50页的数据，也就是一个tag只能访问到1000本书。尽管是通过浏览器访问也是拿不到的。所以直接这样爬肯定有所限制。 豆瓣默认的排序大概加入了评价人数、分数、年份等等因素。我想做的是抛开一切因素，拿到这些数据。 标签分析：给出的大分类有形式、类型、地区、年代、特色五个分类。由于本次爬虫目标是电影，因此形式这里必须是选择电影了。而其他四个分类，各自都有许多标签，那么问题就来了： 如果我每一次请求都带上一个标签，就可以细化每次的请求，是不是可以尽可能拿到更多的数据？ 实现 tags设计 # tags # type包括了全部类型 type = [&#39;剧情&#39;,&#39;喜剧&#39;,&#39;动作&#39;,&#39;爱情&#39;,&#39;科幻&#39;,&#39;动画&#39;,&#39;悬疑&#39;,&#39;惊悚&#39;,&#39;恐怖&#39;,&#39;犯罪&#39;,&#39;同性&#39;,&#39;音乐&#39;,&#39;歌舞&#39;,&#39;传记&#39;,&#39;历史&#39;,&#39;战争&#39;,&#39;西部&#39;,&#39;奇幻&#39;,&#39;冒险&#39;,&#39;灾难&#39;,&#39;武侠&#39;,&#39;情色&#39;] # region包括了全部地区 region = [&#39;中国大陆&#39;,&#39;美国&#39;,&#39;中国香港&#39;,&#39;中国台湾&#39;,&#39;日本&#39;,&#39;韩国&#39;,&#39;英国&#39;,&#39;法国&#39;,&#39;德国&#39;,&#39;意大利&#39;,&#39;西班牙&#39;,&#39;印度&#39;,&#39;泰国&#39;,&#39;俄罗斯&#39;,&#39;伊朗&#39;,&#39;加拿大&#39;,&#39;澳大利亚&#39;,&#39;爱尔兰&#39;,&#39;瑞典&#39;,&#39;巴西&#39;,&#39;丹麦&#39;] # years包括了全部年代（通过80年代等可以判断这个字段接受的是字符串） years = [&#39;2019&#39;,&#39;2018&#39;,&#39;2010年代&#39;,&#39;2000年代&#39;,&#39;90年代&#39;,&#39;80年代&#39;,&#39;70年代&#39;,&#39;60年代&#39;,&#39;更早&#39;] # characteristics 包括了全部特色 characteristics = [&#39;经典&#39;,&#39;青春&#39;,&#39;文艺&#39;,&#39;搞笑&#39;,&#39;励志&#39;,&#39;魔幻&#39;,&#39;感人&#39;,&#39;女性&#39;,&#39;黑帮&#39;] 在设计tags的过程中我发现，在年代的列表中，只有2019年和2018年被区别对待了。原因很容易就能看出来，这两年比较接近现在。那如果想要重点对待某一年或者某一段时间，只需要自己再加上就好了。在这里可以看一下请求的链接形式： url = &#39;https://movie.douban.com/j/new_search_subjects?sort=U&amp;range=0,10&amp;tags=经典&amp;start=0&amp;countries=中国大陆&amp;year_range=2019,2019&#39;请求的URL包括一下几个参数： sort=U: 排序方式，可以选择以下几个： 符号 U S T R 含义 近期热门 评分最高 标记最多 最新上映 range=0,10 : 评分筛选，可以选择筛选的电影的评分区间，这里一般设置为7，10就可以筛选出来大部分好电影。有一个问题在于这里的0，10的数据类型是什么样的； tags=中国大陆： 这里的tags指的是前面设置的type； start=0: 即为偏移量。需要注意的地方是请求链接里不能设置步长，每次请求默认返回20个； countries: 地区，不多解释； year_range=2019,2019: 这个字段很有意思。我们发出的请求是2019，但是通过URL拼接就变成了一个元组一样的东西，表示了年份区间。因此在设置年份的时候可以有以下几个方式： # 年份设置 # 默认设置方式 years = [&#39;2019&#39;,&#39;2018&#39;,&#39;2010年代&#39;,&#39;2000年代&#39;,&#39;90年代&#39;,&#39;80年代&#39;,&#39;70年代&#39;,&#39;60年代&#39;,&#39;更早&#39;] # 生成一个从1960年到2019年的列表并转成字符串，加上一个更早的年份区间 years = [str(i) for i in range(1960,2020)] + [&#39;更早&#39;] # 将需要区别对待的年份单独列出,这里区别对待了90年代所有的年份 years = [&#39;2019&#39;,&#39;2018&#39;,&#39;2010年代&#39;,&#39;2000年代&#39;,&#39;80年代&#39;,&#39;70年代&#39;,&#39;60年代&#39;,&#39;更早&#39;] + [str(i) for i in range(1990,1999)]但是问题出现了：我们虽然在很努力的设置年份，但是忘了一件事情。在构造URL时，接收的参数是两个而不是一个，也就是说，years这个参数，需要两个列表来完成。 也就是说，要写成如下形式： # 先设置前 year1 = [&#39;2019&#39;,&#39;2018&#39;,&#39;2014&#39;,&#39;2010&#39;,&#39;2005&#39;,&#39;2000&#39;,&#39;1995&#39;,&#39;1990&#39;...] # 再设置后 year2 = [&#39;2019&#39;,&#39;2018&#39;,&#39;2017&#39;,&#39;2013&#39;,&#39;2009&#39;,&#39;2004&#39;,&#39;1999&#39;,&#39;1994&#39;...]也就是说，两个需要互相对应，组成年份参数。比如我现在不需要特殊对待这两年，我只需要每隔五年用作一个区间就好了，那就可以用列表生成式。 year1 = [str(i) for i in range(1960,2016,5)] year2 = [str(i) for i in range(1964,2020,5)]生成的结果如下所示： [1960, 1965, 1970, 1975, 1980, 1985, 1990, 1995, 2000, 2005, 2010, 2015] [1964, 1969, 1974, 1979, 1984, 1989, 1994, 1999, 2004, 2009, 2014, 2019] 正好一共12个区间。 好了现在可以生成URL了现在有下面几个参数： sort=T,即按照标记数量排序； range=7,10 即筛选7到10分之间的电影； tags, 按照type列表遍历； start=0, 当然是从0开始，但需要捕捉异常； countries 列表遍历； year1,year2按照两个年份列表遍历。 问题来了——这个问题看上去像是个3层嵌套循环 现在先用列表生成式： # type包括了全部类型 type = [&#39;剧情&#39;,&#39;喜剧&#39;,&#39;动作&#39;,&#39;爱情&#39;,&#39;科幻&#39;,&#39;动画&#39;,&#39;悬疑&#39;,&#39;惊悚&#39;,&#39;恐怖&#39;,&#39;犯罪&#39;,&#39;同性&#39;,&#39;音乐&#39;,&#39;歌舞&#39;,&#39;传记&#39;,&#39;历史&#39;,&#39;战争&#39;,&#39;西部&#39;,&#39;奇幻&#39;,&#39;冒险&#39;,&#39;灾难&#39;,&#39;武侠&#39;,&#39;情色&#39;] # region包括了全部地区 region = [&#39;中国大陆&#39;,&#39;美国&#39;,&#39;中国香港&#39;,&#39;中国台湾&#39;,&#39;日本&#39;,&#39;韩国&#39;,&#39;英国&#39;,&#39;法国&#39;,&#39;德国&#39;,&#39;意大利&#39;,&#39;西班牙&#39;,&#39;印度&#39;,&#39;泰国&#39;,&#39;俄罗斯&#39;,&#39;伊朗&#39;,&#39;加拿大&#39;,&#39;澳大利亚&#39;,&#39;爱尔兰&#39;,&#39;瑞典&#39;,&#39;巴西&#39;,&#39;丹麦&#39;] # 年份设置 year1 = [str(i) for i in range(1960,2016,5)] year2 = [str(i) for i in range(1964,2020,5)] base_url = &#39;https://movie.douban.com/j/new_search_subjects?sort=T&amp;range=7,10&amp;tags=电影&amp;start=0&amp;genres=&#39; # 先拼接type,得到一个长度为22的列表 t_url = [base_url + i for i in type] # 拼接region r_url = [t_url + &#39;&amp;countries=&#39; + i for i in region]好了还没开始就报错了，我像是个傻子. ps:字体设置在这里 这个问题更像是一个排列组合问题但不是。既然用了python那就要想方设法避免循环，更要避免循环嵌套。否则要慢的骂街。 这个问题，稍微计算一下：拼接完type的长度是22；那么拼接完地区，长度应该是22xlen(region)=22x21=462;接下来拼接年份，长度变成462x12 = 5544; 那么拼接就变成了下面这个样子 # 拼接region r_url = [t_url[j] + &#39;&amp;countries=&#39; + i for j in range(0,len(t_url)) for i in region] # 拼接年份 y_url = [r_url[j] + &#39;&amp;year_range=&#39; + year1[i] + &#39;,&#39; + year2[i] for j in range(0,462) for i in range(0,12)] 出现的新的问题，URL中含有中文改怎么解决，现在拼接成功的URL是下面这个样子： https://movie.douban.com/j/new_search_subjects?sort=T&amp;range=7,10&amp;tags=\\xe7\\x94\\xb5\\xe5\\xbd\\xb1&amp;start=0&amp;genres=\\xe5\\x89\\xa7\\xe6\\x83\\x85&amp;countries=\\xe4\\xb8\\xad\\xe5\\x9b\\xbd\\xe5\\xa4\\xa7\\xe9\\x99\\x86&amp;year_range=1960,1964 需要用到下面这种方法： from urllib import parse # type包括了全部类型 type = [&#39;剧情&#39;,&#39;喜剧&#39;,&#39;动作&#39;,&#39;爱情&#39;,&#39;科幻&#39;,&#39;动画&#39;,&#39;悬疑&#39;,&#39;惊悚&#39;,&#39;恐怖&#39;,&#39;犯罪&#39;,&#39;同性&#39;,&#39;音乐&#39;,&#39;歌舞&#39;,&#39;传记&#39;,&#39;历史&#39;,&#39;战争&#39;,&#39;西部&#39;,&#39;奇幻&#39;,&#39;冒险&#39;,&#39;灾难&#39;,&#39;武侠&#39;,&#39;情色&#39;] # region包括了全部地区 region = [&#39;中国大陆&#39;,&#39;美国&#39;,&#39;中国香港&#39;,&#39;中国台湾&#39;,&#39;日本&#39;,&#39;韩国&#39;,&#39;英国&#39;,&#39;法国&#39;,&#39;德国&#39;,&#39;意大利&#39;,&#39;西班牙&#39;,&#39;印度&#39;,&#39;泰国&#39;,&#39;俄罗斯&#39;,&#39;伊朗&#39;,&#39;加拿大&#39;,&#39;澳大利亚&#39;,&#39;爱尔兰&#39;,&#39;瑞典&#39;,&#39;巴西&#39;,&#39;丹麦&#39;] # 年份设置 year1 = [str(i) for i in range(1960,2016,5)] year2 = [str(i) for i in range(1964,2020,5)] base_url = &#39;https://movie.douban.com/j/new_search_subjects?sort=T&amp;range=7,10&amp;tags=电影&amp;start=0&amp;genres=&#39; # 先拼接type,得到一个长度为22的列表 t_url = [base_url + parse.quote(i) for i in type] r_url = [t_url[j] + &#39;&amp;countries=&#39; + parse.quote(i) for j in range(0,len(t_url)) for i in region] # 拼接年份 y_url = [r_url[j] + &#39;&amp;year_range=&#39; + year1[i] + &#39;,&#39; + year2[i] for j in range(0,462) for i in range(0,12)]测试 $ python3 &gt;&gt;&gt; from ..... &gt;&gt;&gt; y_url[0] &#39;https://movie.douban.com/j/new_search_subjects?sort=T&amp;range=7,10&amp;tags=电影&amp;start=0&amp;genres=%E5%89%A7%E6%83%85&amp;countries=%E4%B8%AD%E5%9B%BD%E5%A4%A7%E9%99%86&amp;year_range=1960,1964&#39;BINGO!到此为止生成了5544个URL，下一步就是取了。","categories":[{"name":"爬虫","slug":"爬虫","permalink":"http://xiaoguiwk.xyz/categories/爬虫/"}],"tags":[{"name":"豆瓣","slug":"豆瓣","permalink":"http://xiaoguiwk.xyz/tags/豆瓣/"},{"name":"爬虫","slug":"爬虫","permalink":"http://xiaoguiwk.xyz/tags/爬虫/"},{"name":"异步","slug":"异步","permalink":"http://xiaoguiwk.xyz/tags/异步/"},{"name":"Markdown字体","slug":"Markdown字体","permalink":"http://xiaoguiwk.xyz/tags/Markdown字体/"},{"name":"Markdown表格","slug":"Markdown表格","permalink":"http://xiaoguiwk.xyz/tags/Markdown表格/"},{"name":"Markdown图片","slug":"Markdown图片","permalink":"http://xiaoguiwk.xyz/tags/Markdown图片/"}]},{"title":"服务器笔记","slug":"服务器笔记","date":"2019-08-20T05:44:45.000Z","updated":"2019-08-20T15:59:45.403Z","comments":true,"path":"2019/08/20/服务器笔记/","link":"","permalink":"http://xiaoguiwk.xyz/2019/08/20/服务器笔记/","excerpt":"主要记录Linux系统在使用过程中碰到的问题及一些解决办法。包括各个有可能的方面。","text":"主要记录Linux系统在使用过程中碰到的问题及一些解决办法。包括各个有可能的方面。 连接ssh 输入ssh -p 22 root@101.200.86.233它会提示你输入密码,输入正确的密码之后,你就发现已经登陆成功了.(22: 端口号 root: 用户名)后台运行 Nohup python3 main.py &amp;，输入命令后回车退出 查看输出内容：tail -f nohup.out 关闭：ps -aux | grep “users.py”查看tid，kill tid 查看后台程序：jobs -l（这是L）screen,后台运行的最佳解决方案 创建新的screen：Screen -a x (x是名称) 查看已有screen：Screen -ls Screen -x 6149 恢复pid是6149的screen screen -x user 恢复名称为user的screen screen -X -S 6149 quit 关闭 或者kill 6149即可关闭设置交换空间https://cloud.tencent.com/developer/article/1342505查找文件find / -name php.inivim查找字符串 默认大小写敏感(可以调整)，在normal模式下：esc+:+/+关键字如：$ :/max回车即可。n查找下一个，N查找上一个； 如果默认为大小写敏感，在查询语句后加\\c即为忽略大小写，如：$ :/max\\c即可。","categories":[{"name":"服务器","slug":"服务器","permalink":"http://xiaoguiwk.xyz/categories/服务器/"}],"tags":[{"name":"服务器","slug":"服务器","permalink":"http://xiaoguiwk.xyz/tags/服务器/"},{"name":"ubuntu","slug":"ubuntu","permalink":"http://xiaoguiwk.xyz/tags/ubuntu/"}]},{"title":"diamond","slug":"diamond","date":"2019-08-19T16:10:49.000Z","updated":"2019-08-20T16:08:33.126Z","comments":true,"path":"2019/08/20/diamond/","link":"","permalink":"http://xiaoguiwk.xyz/2019/08/20/diamond/","excerpt":"吹一波Diamond！","text":"吹一波Diamond！","categories":[],"tags":[{"name":"pictures","slug":"pictures","permalink":"http://xiaoguiwk.xyz/tags/pictures/"}]},{"title":"hexo坑与笔记","slug":"hexo坑与笔记","date":"2019-08-18T05:44:45.000Z","updated":"2019-08-18T08:10:07.982Z","comments":true,"path":"2019/08/18/hexo坑与笔记/","link":"","permalink":"http://xiaoguiwk.xyz/2019/08/18/hexo坑与笔记/","excerpt":"hexo安装起来倒是挺快的，可是如果想要高程度自定义和美化自己的博客，很多地方都要下功夫。像我这种不懂得前端代码甚至markdown都写的马马虎虎的人，就有点费劲了。","text":"hexo安装起来倒是挺快的，可是如果想要高程度自定义和美化自己的博客，很多地方都要下功夫。像我这种不懂得前端代码甚至markdown都写的马马虎虎的人，就有点费劲了。 安装整个安装过程非常顺利，根据网上的教程和官网文档来做的，大概是以下几个步骤。1.安装并配置git brew install git2.安装nvm #用来安装Node.js wget -qO- https://raw.githubusercontent.com/nvm-sh/nvm/v0.34.0/install.sh | sh 3.安装Node.js nvm install stable4.安装Hexo npm install -g hexo-cli5.到这里差不多可以启动了，在_config.yml中设置几个参数，就可以启动了。hexo d直接部署。 域名我到腾讯云买了一个域名xiaoguiwk.xyz年付11块钱。没想到还要实名认证，感觉得需要个三四天了。绑定域名有几个步骤：1.在GitHub上的xiaoguiwk.github.io项目中，打开settings，往下拉看到Github Pages，在custom domain中填上自己的域名，save;2.打开腾讯云控制台，解析域名，填上xiaoguiwk.github.io的ip地址。ip的获取方法是ping一下…….这里其实是有问题的，GitHub好像并不会给每个用户一个固定IP地址。有人说可以把自己的域名解析到GitHub的二级域名下，但腾讯云好像并不支持这么做。3.对了根目录要建一个CNAME填上域名。 在跟目录下不行，直接导致的结果就是每次hexo d之后就要上settings页面设置域名。正确的做法是在source目录下创建CNAME文件填上自己的域名。 不过有一点好的是好像是直接就可以用了，可能不直接实名认证的话过几天就上不了了。4.证书设置我还不知道怎么设置。 主题主题采用的是Nexmoe的主题，安装过程比较顺利，主要是安装完成后出现了一点小问题————代码块被识别成为了表格。查来查去没有找到答案，最后在主题GitHub的issue中找到了答案，原来是默认的代码高亮与主题的代码高亮产生了冲突。错误如下图： 解决办法是关掉默认的代码高亮。打开_config.yml修改如下： highlight: enable: false line_number: false auto_detect: false tab_replace:文章封面图文章封面图的添加比较简单，在创建文章之前找好一张图，记下图片地址。 $ hexo n &quot;hexo坑与笔记&quot;然后打开文章，在顶部添加cover。 --- title: hexo坑与笔记 date: 2019-08-18 13:44:45 tags: hexo theme cover: https://images6.alphacoders.com/766/766327.jpg ---评论系统livere易于设置，步骤如下： 注册livere帐号； 安装City版本； 复制uid到主题配置文件_config.yml中: comment: livere livere: data_uid: MTAyMC80NjE0OC8yMjY1OQ== 域名实名认证成功后记得改自己的域名。 站长统计如下： analytics: # 统计系统，目前支持 Google analytics.js 统计、Google Tag Manager 统计、CNZZ 统计、腾讯统计、51.La统计、百度统计 google_site_id: #&lt;ID&gt; gtags_site_id: #&lt;ID&gt; cnzz_site_id: 1277935893 tencent_site_id: #&lt;ID&gt; la_site_id: #&lt;ID&gt; baidu_site_id: #&lt;ID&gt; gtm_container_id: #&lt;ID&gt;域名实名认证成功后记得改自己的域名。 头像与网站iconavatar: /images/avatar.png # 网站 Logo background: https://i.loli.net/2019/01/13/5c3aec85a4343.jpg # 既是博客的背景，又是文章默认头图 favicon: href: /images/favicon.ico # 网站图标 type: image/png # 图标类型，可能的值有(image/png, image/vnd.microsoft.icon, image/x-icon, image/gif)其中，avatar.png是头像，favicon.ico是网站图标。 插入图片注意事项图片插入的格式为: {% img /images/hexo坑与笔记/1.png \"代码块错误\" %} {% img}是固定的； /images/1.png 是图片的地址，images前面一定要加/，images文件夹放在source文件夹下面； “代码块错误”是图片描述，图挂了会显示 tags设置这种方法是成功不了的。。。 tags: themes hexo正确的方法应该是这样设置tags： tags: - themes - hexomarkdown语法坑 不会就赶紧去学，先照着这个https://www.jianshu.com/p/191d1e21f7ed； 列表输入完成之后一定要空行，不然后面全都缩进了； 也就是说这一行和上一行在编辑的时候中间有一个空行。 markdown的插入图片的方式不适用，如果想用的话需要安装插件。 代码块的插入方式有两种，一种是反引号引用，另一种是codeblock，如下： {% codeblock lang:objc %} [rectangle setX: 10 y: 10 width: 20 height: 20]; {% endcodeblock %}其中，lang是指定语言。 暂时先记这么多吧。","categories":[{"name":"hexo","slug":"hexo","permalink":"http://xiaoguiwk.xyz/categories/hexo/"}],"tags":[{"name":"themes","slug":"themes","permalink":"http://xiaoguiwk.xyz/tags/themes/"},{"name":"hexo","slug":"hexo","permalink":"http://xiaoguiwk.xyz/tags/hexo/"}]},{"title":"服务器环境配置","slug":"服务器环境配置","date":"2019-08-17T13:06:58.000Z","updated":"2019-08-18T06:34:51.810Z","comments":true,"path":"2019/08/17/服务器环境配置/","link":"","permalink":"http://xiaoguiwk.xyz/2019/08/17/服务器环境配置/","excerpt":"一台新租的服务器需要更新、配置常用环境。把这些事情记录下来以后就可以有个样板了。","text":"一台新租的服务器需要更新、配置常用环境。把这些事情记录下来以后就可以有个样板了。 1.更新$ su #获取管理员权限 password: #输入密码 $ apt-get update #更新软件列表 $ apt-get upgrade #更新软件 2.安装redis2.1安装Redis$ apt-get install redis-server -y #安装redis服务器 $ redis-server -v #查看redis版本 2.2 启动redis$ redis-server #启动redis服务器 $ redis-cli #启动redis客户端 127.0.0.1:6379&gt; ping PONG #即为连接成功2.3 后台运行1）修改配置文件 $ vim /etc/redis/redis.conf 将daemonize no 改为 yes $ redis-server redis.conf 结果发现不好使；2）使用守护进程 $ redis-server &amp; &lt;打印信息&gt; Ctrl+C即可后台运行 2.4 关闭redis-cli shutdown 3.错误处理Errors were encountered while processing: redis-server E: Sub-process /usr/bin/dpkg returned an error code (1) 解决办法： $ cd /var/lib/dpkg $ sudo mv info info.bak $ sudo mkdir info $ sudo apt-get upgrade 4.更新Python环境$ apt-get upgrade python3 3.6提示不用更新。。。 5.Git6.Python 安装pip $ apt install python3-piprequests bs4 pandas numpy scrapy aiohttp pymysql(需要先安装mysql) 7.Lnmp8.docker8.1 安装 安装以下包以使apt可以通过HTTPS使用存储库（repository）： $ sudo apt-get install -y apt-transport-https ca-certificates curl software-properties-common 添加Docker官方的GPG密钥： $ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - 使用下面的命令来设置stable存储库： $ sudo add-apt-repository &quot;deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable&quot; 再更新一下apt包索引： $ sudo apt-get update 安装最新版本的Docker CE： $ sudo apt-get install -y docker-ce8.2 启动 查看docker服务是否启动： $ systemctl status docker 即为已启动，如未启动， $ sudo systemctl start docker 验证docker服务： sudo docker run hello-world","categories":[],"tags":[{"name":"服务器","slug":"服务器","permalink":"http://xiaoguiwk.xyz/tags/服务器/"},{"name":"环境配置","slug":"环境配置","permalink":"http://xiaoguiwk.xyz/tags/环境配置/"},{"name":"ubuntu","slug":"ubuntu","permalink":"http://xiaoguiwk.xyz/tags/ubuntu/"}]},{"title":"Hello World","slug":"hello-world","date":"2019-08-17T12:10:43.325Z","updated":"2019-08-22T15:40:53.029Z","comments":true,"path":"2019/08/17/hello-world/","link":"","permalink":"http://xiaoguiwk.xyz/2019/08/17/hello-world/","excerpt":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub.","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post$ hexo new &quot;My New Post&quot; More info: Writing Run server$ hexo server More info: Server Generate static files$ hexo generate More info: Generating Deploy to remote sites$ hexo deploy More info: Deployment","categories":[],"tags":[]}]}